{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traditional Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Config the matplotlib backend as plotting inline in IPython\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[matrix([[-2. ,  4.5,  6.2]]), matrix([[  2.6 ,  10.4 ,  12.44]])]\n",
      "[[  7.   9.  18.]]\n",
      "[[ 4.4  -1.4   5.56]]\n",
      "[[ 52.2336]]\n",
      "(1, 1)\n",
      "[None, matrix([[ 1.2]])]\n",
      "(1, 3)\n",
      "[None, matrix([[ 4.4 , -1.4 ,  5.56]])]\n",
      "[None, matrix([[ 19.372]])]\n",
      "[None, matrix([[ 8.56]])]\n"
     ]
    }
   ],
   "source": [
    "#Data\n",
    "x=np.matrix([[-2.0, 4.5, 6.2]])\n",
    "y=np.matrix([[ 7.0, 9.0,18.0]])\n",
    "N=x.shape[1]\n",
    "#Input dimension\n",
    "DI=x.shape[0]\n",
    "#Output dimension\n",
    "DO=y.shape[0]\n",
    "W=[]\n",
    "#weights\n",
    "W.append(None)#not weights input layer\n",
    "#Gradient of cost with respect to weights\n",
    "dL_W=[]\n",
    "dL_W.append(None)#not weights gradient input layer\n",
    "#bias\n",
    "b=[]\n",
    "b.append(None)#not bias for input layer\n",
    "dL_b=[]\n",
    "dL_b.append(None)#not bias gradient for input layer\n",
    "#Activation Layers\n",
    "a=[]\n",
    "a.append(x)# a[0]=x -> input layer\n",
    "#Activation function Layers\n",
    "af=[]\n",
    "af.append(None)#not activation function for input layer\n",
    "#Activation function derivative Layers\n",
    "dL_af=[]\n",
    "dL_af.append(None)#not activation function derivative for input layer\n",
    "#Linear operation Layers\n",
    "l=[]\n",
    "l.append(None)#not linear activation for input layer\n",
    "#Delta Layers\n",
    "d=[]\n",
    "d.append(None)#not delta for input layer\n",
    "\n",
    "#Activation functions\n",
    "#Non linear function\n",
    "def S(l):\n",
    "    #s=1.0/(1+np.exp(-l))\n",
    "    return l\n",
    "def dS(x):\n",
    "    s=S(x)\n",
    "    #ds=s*(1-s)\n",
    "    return 1.0\n",
    "\n",
    "#Units number for layers\n",
    "ln=[DI,DO]\n",
    "af_param=[]\n",
    "af_param.append(None)#not activation function for input layer parameter\n",
    "af_param.append(S)#activation function of layer 1 is S\n",
    "dL_af_param=[]\n",
    "dL_af_param.append(None)#not activation function derivative for input layer parameter\n",
    "dL_af_param.append(dS)#activation function derivative of layer 1 is dS\n",
    "\n",
    "#last layer index\n",
    "L=len(ln)-1\n",
    "for i in range(1,len(ln)):\n",
    "    j=i-1\n",
    "    a.append   (np.matrix(np.zeros((ln[i],N))))\n",
    "    l.append   (np.matrix(np.zeros((ln[i],N))))\n",
    "    d.append   (np.matrix(np.zeros((ln[i],N))))\n",
    "    W.append   (np.matrix(1.0/np.sqrt(ln[j])*np.random.randn(ln[i],ln[j])))\n",
    "    dL_W.append(np.matrix(np.zeros((ln[i],ln[j]))))\n",
    "    b.append   (np.matrix(np.zeros((ln[i],1))))\n",
    "    dL_b.append(np.matrix(np.zeros((ln[i],1))))\n",
    "    af.append(af_param[i])\n",
    "    dL_af.append(dL_af_param[i])\n",
    "    \n",
    "#Values to test\n",
    "W[1]=np.matrix([[1.2]])\n",
    "b[1]=np.matrix([[5.0]])\n",
    "\n",
    "\n",
    "def forwardpass(a,W,b):\n",
    "    for i in range(1,len(l)):\n",
    "        j=i-1\n",
    "        l[i]=W[i]*a[j]+b[i]\n",
    "        #call activation function of layer i\n",
    "        a[i]=af[i](l[i])\n",
    "        \n",
    "def backwardpass(a,y,W,b):\n",
    "    #Delta error\n",
    "    #just up to layer 1\n",
    "    d[L]=(y-a[L])*dL_af[L](l[L])\n",
    "    for i in reversed(range(2,len(ln))):\n",
    "        j=i-1\n",
    "        #call activation funcion derivative of layer i or j?\n",
    "        d[j]=dL_af[i](l[j])*W[i].T*d[i] \n",
    "    #Gradients\n",
    "    for i in reversed(range(1,len(ln))):\n",
    "        j=i-1\n",
    "        dL_W[i]=d[i]*a[j].T#outer product\n",
    "        dL_b[i]=d[i]*np.ones((1,N)).T#np.sum(d[i],axis=1)\n",
    "        \n",
    "forwardpass(a,W,b)\n",
    "print(a)\n",
    "dif=y-a[L]\n",
    "print(y)\n",
    "print(dif)\n",
    "print(dif*dif.T)\n",
    "print(W[1].shape)      \n",
    "print(W)\n",
    "backwardpass(a,y,W,b)\n",
    "print(d[1].shape)\n",
    "print(d)\n",
    "print(dL_W)\n",
    "print(dL_b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Learning rate\n",
    "alpha=0.015\n",
    "def GDstep():\n",
    "    forwardpass(a,W,b)\n",
    "    backwardpass(a,y,W,b)\n",
    "    #Update weights\n",
    "    for i in range(1,len(ln)):\n",
    "        W[i]+=alpha*dL_W[i]\n",
    "        b[i]+=alpha*dL_b[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 27.01599908]]\n",
      "[None, matrix([[ 1.05660282]])]\n",
      "[None, matrix([[ 8.26038385]])]\n",
      "[[ 27.01582234]]\n",
      "[None, matrix([[ 1.05609991]])]\n",
      "[None, matrix([[ 8.26390607]])]\n",
      "[[ 27.01571877]]\n",
      "[None, matrix([[ 1.05571492]])]\n",
      "[None, matrix([[ 8.26660239]])]\n",
      "[[ 27.01565808]]\n",
      "[None, matrix([[ 1.05542021]])]\n",
      "[None, matrix([[ 8.26866645]])]\n",
      "[[ 27.01562251]]\n",
      "[None, matrix([[ 1.05519461]])]\n",
      "[None, matrix([[ 8.27024652]])]\n",
      "[[ 27.01560167]]\n",
      "[None, matrix([[ 1.0550219]])]\n",
      "[None, matrix([[ 8.27145609]])]\n",
      "[[ 27.01558946]]\n",
      "[None, matrix([[ 1.0548897]])]\n",
      "[None, matrix([[ 8.27238202]])]\n",
      "[[ 27.0155823]]\n",
      "[None, matrix([[ 1.05478849]])]\n",
      "[None, matrix([[ 8.27309084]])]\n",
      "[[ 27.0155781]]\n",
      "[None, matrix([[ 1.05471102]])]\n",
      "[None, matrix([[ 8.27363345]])]\n",
      "[[ 27.01557565]]\n",
      "[None, matrix([[ 1.05465171]])]\n",
      "[None, matrix([[ 8.27404883]])]\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    GDstep()\n",
    "    if i%10==0:\n",
    "        #Loss difference squared\n",
    "        dif=y-a[L]\n",
    "        print(dif*dif.T)\n",
    "        #Weights\n",
    "        print(W)\n",
    "        #Bias\n",
    "        print(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
