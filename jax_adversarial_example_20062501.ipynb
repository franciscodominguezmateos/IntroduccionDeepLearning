{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Francisco Dominguez Mateos\n",
    "# 25/06/2020\n",
    "# Creating Adversarial Examples for Neural Networks with JAX\n",
    "# from: https://towardsdatascience.com/creating-adversarial-examples-with-jax-from-the-scratch-bf267757f672"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import array\n",
    "import gzip\n",
    "import itertools\n",
    "import numpy\n",
    "import numpy.random as npr\n",
    "import os\n",
    "import struct\n",
    "import time\n",
    "from os import path\n",
    "import urllib.request\n",
    "import jax.numpy as np\n",
    "from jax.api import jit, grad\n",
    "from jax.config import config\n",
    "from jax.scipy.special import logsumexp\n",
    "from jax import random\n",
    "import matplotlib.pyplot as plt \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial Examples\n",
    "Adversarial Examples are inputs to a neural network that are optimized to fool the algorithm.\n",
    "\n",
    "We are going to use fast gradient sign method.\n",
    "\n",
    "The trick is just generate adversarial samples with this equation:\n",
    "\n",
    "$$x_a=x_o+\\epsilonÂ·sign(\\nabla_{x_f} J(\\theta,x_f,y))$$\n",
    "\n",
    "where the adversarial image $x_a$ is obtained by taking the sign of the gradient of cross-entropy loss, J, w.r.t input image $x_f$ and adding it to the original image x_o. $\\epsilon$ is the hyperparameter here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function for calculating predictions and accuracy before pertubation\n",
    "# grad(loss)(params,batch) take derivative as usual on params\n",
    "def loss(params, batch, test=0):\n",
    "  inputs, targets = batch\n",
    "  logits = predict(params, inputs)\n",
    "  preds  = stax.logsoftmax(logits)\n",
    "  if(test==1):\n",
    "    print('Prediction Vector before softmax')\n",
    "    print(logits)\n",
    "    print(\"____________________________________________________________________________________\")\n",
    "    print('Prediction Vector after softmax')\n",
    "    print(preds)\n",
    "    print(\"____________________________________________________________________________________\")\n",
    "  return -(1/(preds.shape[0]))*np.sum(targets*preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The traditional gradient is computed in JAX as: (Notice that loss() in JAX is equal to J() in math)\n",
    "\n",
    "$$grad(loss)(params,batch)==\\nabla_{params}J(params,batch)$$\n",
    "\n",
    "\n",
    "and the gradien with respect to the input is: (Notice in this case that lo() in JAX is equal to J() in math)\n",
    "\n",
    "$$grad(lo)(batch,params)==\\nabla_{batch} J(batch,params)$$\n",
    "\n",
    "\n",
    "since JAX take derivative only on first parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function for calculating gradients of loss w.r.t. input image\n",
    "# because of the order grad(lo)(batch,params) take derivative on batch not on params\n",
    "def lo(batch,params):\n",
    "  inputs, targets = batch\n",
    "  logits = predict(params, inputs)\n",
    "  preds  = stax.logsoftmax(logits)\n",
    "  return -(1/(preds.shape[0]))*np.sum(targets*preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
