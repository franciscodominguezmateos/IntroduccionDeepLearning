{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Francisco Dominguez Mateos\n",
    "# 25/06/2020\n",
    "# from: https://github.com/google/jax/blob/master/examples/advi.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2018 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\"\"\"Automatic differentiation variational inference in Numpy and JAX.\n",
    "This demo fits a Gaussian approximation to an intractable, unnormalized\n",
    "density, by differentiating through a Monte Carlo estimate of the\n",
    "variational evidence lower bound (ELBO).\"\"\"\n",
    "\n",
    "\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from jax.api import jit, grad, vmap\n",
    "from jax import random\n",
    "from jax.experimental import optimizers\n",
    "import jax.numpy as jnp\n",
    "import jax.scipy.stats.norm as norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= Functions to define the evidence lower bound. =========\n",
    "\n",
    "def diag_gaussian_sample(rng, mean, log_std):\n",
    "    # Take a single sample from a diagonal multivariate Gaussian.\n",
    "    return mean + jnp.exp(log_std) * random.normal(rng, mean.shape)\n",
    "\n",
    "def diag_gaussian_logpdf(x, mean, log_std):\n",
    "    # Evaluate a single point on a diagonal multivariate Gaussian.\n",
    "    return jnp.sum(vmap(norm.logpdf)(x, mean, jnp.exp(log_std)))\n",
    "\n",
    "def elbo(logprob, rng, mean, log_std):\n",
    "    # Single-sample Monte Carlo estimate of the variational lower bound.\n",
    "    sample = diag_gaussian_sample(rng, mean, log_std)\n",
    "    return logprob(sample) - diag_gaussian_logpdf(sample, mean, log_std)\n",
    "\n",
    "def batch_elbo(logprob, rng, params, num_samples):\n",
    "    # Average over a batch of random samples.\n",
    "    rngs = random.split(rng, num_samples)\n",
    "    vectorized_elbo = vmap(partial(elbo, logprob), in_axes=(0, None, None))\n",
    "    return jnp.mean(vectorized_elbo(rngs, *params))\n",
    "\n",
    "\n",
    "# ========= Helper function for plotting. =========\n",
    "\n",
    "@partial(jit, static_argnums=(0, 1, 2, 4))\n",
    "def _mesh_eval(func, x_limits, y_limits, params, num_ticks):\n",
    "    # Evaluate func on a 2D grid defined by x_limits and y_limits.\n",
    "    x = jnp.linspace(*x_limits, num=num_ticks)\n",
    "    y = jnp.linspace(*y_limits, num=num_ticks)\n",
    "    X, Y = jnp.meshgrid(x, y)\n",
    "    xy_vec = jnp.stack([X.ravel(), Y.ravel()]).T\n",
    "    zs = vmap(func, in_axes=(0, None))(xy_vec, params)\n",
    "    return X, Y, zs.reshape(X.shape)\n",
    "\n",
    "def mesh_eval(func, x_limits, y_limits, params, num_ticks=101):\n",
    "    return _mesh_eval(func, x_limits, y_limits, params, num_ticks)\n",
    "\n",
    "# ========= Define an intractable unnormalized density =========\n",
    "\n",
    "def funnel_log_density(params):\n",
    "    return norm.logpdf(params[0], 0, jnp.exp(params[1])) + \\\n",
    "           norm.logpdf(params[1], 0, 1.35)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    num_samples = 40\n",
    "\n",
    "    @jit\n",
    "    def objective(params, t):\n",
    "        rng = random.PRNGKey(t)\n",
    "        return -batch_elbo(funnel_log_density, rng, params, num_samples)\n",
    "\n",
    "    # Set up figure.\n",
    "    fig = plt.figure(figsize=(8,8), facecolor='white')\n",
    "    ax = fig.add_subplot(111, frameon=False)\n",
    "    plt.ion()\n",
    "    plt.show(block=False)\n",
    "    x_limits = [-2, 2]\n",
    "    y_limits = [-4, 2]\n",
    "    target_dist = lambda x, _: jnp.exp(funnel_log_density(x))\n",
    "    approx_dist = lambda x, params: jnp.exp(diag_gaussian_logpdf(x, *params))\n",
    "\n",
    "    def callback(params, t):\n",
    "        print(\"Iteration {} lower bound {}\".format(t, objective(params, t)))\n",
    "\n",
    "        plt.cla()\n",
    "        X, Y, Z = mesh_eval(target_dist, x_limits, y_limits, 1)\n",
    "        ax.contour(X, Y, Z, cmap='summer')\n",
    "        X, Y, Z = mesh_eval(approx_dist, x_limits, y_limits, params)\n",
    "        ax.contour(X, Y, Z, cmap='winter')\n",
    "        ax.set_xlim(x_limits)\n",
    "        ax.set_ylim(y_limits)\n",
    "        ax.set_yticks([])\n",
    "        ax.set_xticks([])\n",
    "\n",
    "        # Plot random samples from variational distribution.\n",
    "        # Here we clone the rng used in computing the objective\n",
    "        # so that we can show exactly the same samples.\n",
    "        rngs = random.split(random.PRNGKey(t), num_samples)\n",
    "        samples = vmap(diag_gaussian_sample, in_axes=(0, None, None))(rngs, *params)\n",
    "        ax.plot(samples[:, 0], samples[:, 1], 'b.')\n",
    "\n",
    "        plt.draw()\n",
    "        plt.pause(1.0/60.0)\n",
    "\n",
    "\n",
    "    # Set up optimizer.\n",
    "    D = 2\n",
    "    init_mean = jnp.zeros(D)\n",
    "    init_std  = jnp.zeros(D)\n",
    "    init_params = (init_mean, init_std)\n",
    "    opt_init, opt_update, get_params = optimizers.momentum(step_size=0.1, mass=0.9)\n",
    "    opt_state = opt_init(init_params)\n",
    "\n",
    "    @jit\n",
    "    def update(i, opt_state):\n",
    "        params = get_params(opt_state)\n",
    "        gradient = grad(objective)(params, i)\n",
    "        return opt_update(i, gradient, opt_state)\n",
    "\n",
    "\n",
    "    # Main loop.\n",
    "    print(\"Optimizing variational parameters...\")\n",
    "    for t in range(100):\n",
    "        opt_state = update(t, opt_state)\n",
    "        params = get_params(opt_state)\n",
    "        callback(params, t)\n",
    "    plt.show(block=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
