{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "royal-count",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Francisco Dominguez Mateos\n",
    "# 09/05/2021\n",
    "# Inspired by: https://www.tensorflow.org/tutorials/text/transformer\n",
    "# Playing with attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "departmental-scotland",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from jax import random\n",
    "import jax.numpy as np\n",
    "from jax.experimental.stax import Dense, serial, Relu, BatchNorm, Dropout\n",
    "from jax.nn.initializers import uniform\n",
    "from jax.ops import index_update\n",
    "# Current convention is to import original numpy as \"onp\"\n",
    "import numpy as onp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "congressional-service",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate key which is used to generate random numbers\n",
    "rng = random.PRNGKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "interim-surfing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(k,q,v):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    kT=np.swapaxes(k,-1,-2)\n",
    "    #print(\"q =\",q.shape)\n",
    "    #print(\"kT=\",kT.shape)\n",
    "    qk=np.matmul(q,kT)\n",
    "    #print(\"qk=\",qk.shape)\n",
    "    dk=k.shape[-1]\n",
    "    scalled_attention_logits=qk/np.sqrt(dk)\n",
    "    attention_weights=jax.nn.softmax(scalled_attention_logits)\n",
    "    #print(\"attention_weights=\",attention_weights.shape)\n",
    "    output=np.matmul(attention_weights,v)\n",
    "    return output, attention_weights\n",
    "def attention_scaled_dot_product(k,q,v,mask=None):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    kT=np.swapaxes(k,-1,-2)\n",
    "    #print(\"q =\",q.shape)\n",
    "    #print(\"kT=\",kT.shape)\n",
    "    qk=np.matmul(q,kT)\n",
    "    #print(\"qk=\",qk.shape)\n",
    "    dk=k.shape[-1]\n",
    "    scalled_attention_logits=qk/np.sqrt(dk)\n",
    "    #TODO: apply the mask\n",
    "    attention_weights=jax.nn.softmax(scalled_attention_logits)\n",
    "    #print(\"attention_weights=\",attention_weights.shape)\n",
    "    output=np.matmul(attention_weights,v)\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "binding-commonwealth",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.2f}\".format(x)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "rational-trail",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a= [[0.00 1.00 0.00 0.00]]\n",
      "o= [[10.00 0.00]]\n"
     ]
    }
   ],
   "source": [
    "temp_k = np.array([[10, 0, 0],\n",
    "                      [0, 10, 0],\n",
    "                      [0, 0, 10],\n",
    "                      [0, 0, 10]])  # (4, 3)\n",
    "\n",
    "temp_v = np.array([[1, 0],\n",
    "                      [10, 0],\n",
    "                      [100, 5],\n",
    "                      [1000, 6]])  # (4, 2)\n",
    "\n",
    "# This `query` aligns with the second `key`,\n",
    "# so the second `value` is returned.\n",
    "temp_q = np.array([[0, 10, 0]])  # (1, 3)\n",
    "o,a=attention_scaled_dot_product(temp_k,temp_q,temp_v)\n",
    "print(\"a=\",a)\n",
    "print(\"o=\",o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "original-graphic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a= [[0.00 0.00 0.50 0.50]]\n",
      "o= [[550.00 5.50]]\n"
     ]
    }
   ],
   "source": [
    "temp_q = np.array([[0, 0, 10]])  # (1, 3)\n",
    "o,a=attention_scaled_dot_product(temp_k,temp_q,temp_v)\n",
    "print(\"a=\",a)\n",
    "print(\"o=\",o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "elect-toyota",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a= [[0.50 0.50 0.00 0.00]]\n",
      "o= [[5.50 0.00]]\n"
     ]
    }
   ],
   "source": [
    "temp_q = np.array([[10, 10, 0]])  # (1, 3)\n",
    "o,a=attention_scaled_dot_product(temp_k,temp_q,temp_v)\n",
    "print(\"a=\",a)\n",
    "print(\"o=\",o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "humanitarian-depth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "[[0.00 0.00 0.50 0.50]\n",
      " [0.00 1.00 0.00 0.00]\n",
      " [0.50 0.50 0.00 0.00]]\n",
      "o=\n",
      "[[550.00 5.50]\n",
      " [10.00 0.00]\n",
      " [5.50 0.00]]\n"
     ]
    }
   ],
   "source": [
    "temp_q = np.array([[0, 0, 10],\n",
    "                      [0, 10, 0],\n",
    "                      [10, 10, 0]])  # (3, 3)\n",
    "o,a=attention_scaled_dot_product(temp_k,temp_q,temp_v)\n",
    "print(\"a=\")\n",
    "print(a)\n",
    "print(\"o=\")\n",
    "print(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "cathedral-nicaragua",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MultiHeadLayer(embed_dim, num_heads=8):\n",
    "    assert embed_dim % num_heads==0\n",
    "    depth=embed_dim//num_heads\n",
    "    init_wk ,wk =Dense(embed_dim)\n",
    "    init_wq ,wq =Dense(embed_dim)\n",
    "    init_wv ,wv =Dense(embed_dim)\n",
    "    init_lin,lin=Dense(embed_dim)\n",
    "    \n",
    "    def init_fun(rng,input_shape):\n",
    "        rng_k,rng_q,rng_v,rng_lin=random.split(rng,4)\n",
    "        shape, param_k   =init_wk (rng_k  ,input_shape)\n",
    "        shape, param_q   =init_wq (rng_q  ,input_shape)\n",
    "        shape, param_v   =init_wv (rng_v  ,input_shape)\n",
    "        shape, param_lin =init_lin(rng_lin,input_shape)\n",
    "        return shape,(param_k,param_q,param_v,param_lin)\n",
    "\n",
    "    def split_heads(x,batch_size):\n",
    "        x=np.reshape(x,(batch_size,-1,num_heads,depth))\n",
    "        x=np.transpose(x,(0,2,1,3))\n",
    "        return x\n",
    "    \n",
    "    def apply_fun(params,x,mask=None):\n",
    "        batch_size=x[0].shape[0]\n",
    "        k,q,v=x\n",
    "        param_k,param_q,param_v,param_lin=params\n",
    "        k=wk(param_k,k)\n",
    "        q=wq(param_q,q)\n",
    "        v=wv(param_v,v)\n",
    "        #print(\"k=\",k.shape)\n",
    "        #print(\"q=\",q.shape)\n",
    "        #print(\"v=\",v.shape)\n",
    "        k=split_heads(k,batch_size)\n",
    "        q=split_heads(q,batch_size)\n",
    "        v=split_heads(v,batch_size)\n",
    "        #print(\"k=\",k.shape)\n",
    "        #print(\"q=\",q.shape)\n",
    "        #print(\"v=\",v.shape)\n",
    "        a,aw=attention_scaled_dot_product(k,q,v)\n",
    "        sa=np.transpose(a,(0,2,1,3))\n",
    "        ca=np.reshape(sa,(batch_size,-1,embed_dim))\n",
    "        output=lin(param_lin,ca)\n",
    "        return output,aw\n",
    "    \n",
    "    return init_fun,apply_fun\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "married-hypothetical",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_shape= (1, 60, 512)\n",
      "o = (1, 60, 512)\n",
      "aw= (1, 8, 60, 60)\n"
     ]
    }
   ],
   "source": [
    "in_shape = (1,60,512)\n",
    "init_mha,mha=MultiHeadLayer(in_shape[2])\n",
    "output_shape, params = init_mha(rng, in_shape)\n",
    "fake_data=random.uniform(rng,in_shape)\n",
    "print(\"output_shape=\",output_shape)\n",
    "o,aw=mha(params,(fake_data,fake_data,fake_data))\n",
    "print(\"o =\",o.shape)\n",
    "print(\"aw=\",aw.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "proprietary-watershed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp_a= (2, 4, 3)\n",
      "[[[0.00 0.00 1.00]\n",
      "  [0.00 1.00 0.00]\n",
      "  [0.00 1.00 0.00]\n",
      "  [1.00 1.00 0.00]]\n",
      "\n",
      " [[0.00 0.00 -1.00]\n",
      "  [0.00 -1.00 0.00]\n",
      "  [0.00 -1.00 0.00]\n",
      "  [-1.00 -1.00 0.00]]]\n",
      "[[[-0.58 -1.73 1.73]\n",
      "  [-0.58 0.58 -0.58]\n",
      "  [-0.58 0.58 -0.58]\n",
      "  [1.73 0.58 -0.58]]\n",
      "\n",
      " [[-0.58 -1.71 1.73]\n",
      "  [-0.58 0.43 -0.58]\n",
      "  [-0.58 0.85 -0.58]\n",
      "  [1.73 0.43 -0.58]]]\n",
      "[[[-0.71 -0.71 1.41]\n",
      "  [-0.71 1.41 -0.71]\n",
      "  [-0.71 1.41 -0.71]\n",
      "  [0.71 0.71 -1.41]]\n",
      "\n",
      " [[-0.71 -0.71 1.41]\n",
      "  [-0.71 1.41 -0.71]\n",
      "  [-0.71 1.41 -0.71]\n",
      "  [0.71 0.71 -1.41]]]\n",
      "[[[-0.85 -0.85 1.18]\n",
      "  [-0.85 1.18 -0.85]\n",
      "  [-0.85 1.18 -0.85]\n",
      "  [1.18 1.18 -0.85]]\n",
      "\n",
      " [[-0.84 -0.84 1.10]\n",
      "  [-0.84 1.10 -0.84]\n",
      "  [-0.84 1.49 -0.84]\n",
      "  [1.10 1.10 -0.84]]]\n"
     ]
    }
   ],
   "source": [
    "#Normalizing ..... to implement layer normalization\n",
    "temp_a = np.array([[[0, 0, 10],\n",
    "                   [0, 10, 0],\n",
    "                   [0, 10, 0],\n",
    "                   [10, 10, 0]],\n",
    "                  [[0, 0, 5],\n",
    "                   [0, 5, 0],\n",
    "                   [0, 6, 0],\n",
    "                   [5, 5, 0]]])\n",
    "print(\"temp_a=\",temp_a.shape)\n",
    "print(jax.nn.normalize(temp_a,axis=0))\n",
    "print(jax.nn.normalize(temp_a,axis=1))\n",
    "print(jax.nn.normalize(temp_a,axis=2))\n",
    "print(jax.nn.normalize(temp_a,axis=(1,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "caring-template",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PointWiseFeedForwardNetwork(embeb_dim,dff):\n",
    "    return serial(Dense(dff),Relu,Dense(embeb_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "chubby-soldier",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_shape= (64, 50, 512)\n",
      "o = (64, 50, 512)\n"
     ]
    }
   ],
   "source": [
    "in_shape = (64,50,512)\n",
    "init_dff,dff=PointWiseFeedForwardNetwork(512,2048)\n",
    "output_shape, params = init_dff(rng, in_shape)\n",
    "fake_data=random.uniform(rng,in_shape)\n",
    "print(\"output_shape=\",output_shape)\n",
    "o=dff(params,fake_data)\n",
    "print(\"o =\",o.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "wooden-commission",
   "metadata": {},
   "outputs": [],
   "source": [
    "def EncoderLayer(embed_dim,num_heads,dff,rate=0.1):\n",
    "    init_mha,mha=MultiHeadLayer(embed_dim,num_heads)\n",
    "    init_ffn,ffn=PointWiseFeedForwardNetwork(embed_dim,dff)\n",
    "    init_ln1,ln1=BatchNorm()\n",
    "    init_ln2,ln2=BatchNorm()\n",
    "    init_do1,do1=Dropout(rate)\n",
    "    init_do2,do2=Dropout(rate)\n",
    "    def init_fun(rnd,input_shape):\n",
    "        rng_mha,rng_ffn,rng_ln1,rng_ln2,rng_do1,rng_do2=random.split(rng,6)\n",
    "        shape, param_mha   =init_mha (rng_mha  ,input_shape)\n",
    "        shape, param_ffn   =init_ffn (rng_ffn  ,input_shape)\n",
    "        shape, param_ln1   =init_ln1 (rng_ln1  ,input_shape)\n",
    "        shape, param_ln2   =init_ln2 (rng_ln2  ,input_shape)\n",
    "        shape, param_do1   =init_do1 (rng_do1  ,input_shape)\n",
    "        shape, param_do2   =init_do2 (rng_do2  ,input_shape)\n",
    "        return in_shape,(param_mha,param_ffn,param_ln1,param_ln2,param_do1,param_do2)\n",
    "    def apply_fun(params,x,rng,training=True,mask=None):\n",
    "        param_mha,param_ffn,param_ln1,param_ln2,param_do1,param_do2=params\n",
    "        attn_output, _=mha(param_mha,(x,x,x),mask)\n",
    "        attn_output=do1(param_do1,attn_output,rng=rng,mode=training)\n",
    "        #print(\"x=\",x.shape)\n",
    "        #print(\"attn_output\",attn_output.shape)\n",
    "        out1=ln1(param_ln1,x+attn_output)\n",
    "        \n",
    "        ffn_output=ffn(param_ffn,out1)\n",
    "        ffn_output=do2(param_do2,ffn_output,rng=rng,mode=training)\n",
    "        out2=ln2(param_ln2,out1+ffn_output)\n",
    "        return out2\n",
    "    return init_fun,apply_fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "secret-frontier",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_shape= (64, 43, 512)\n",
      "oen= (64, 43, 512)\n"
     ]
    }
   ],
   "source": [
    "in_shape = (64,43,512)\n",
    "init_enl,enl=EncoderLayer(512,8,2048)\n",
    "output_shape, params = init_enl(rng, in_shape)\n",
    "fake_data=random.uniform(rng,in_shape)\n",
    "print(\"output_shape=\",output_shape)\n",
    "oen=enl(params,fake_data,rng)\n",
    "print(\"oen=\",oen.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "smaller-shadow",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DecoderLayer(embed_dim,num_heads,dff,rate=0.1):\n",
    "    init_ma1,ma1=MultiHeadLayer(embed_dim,num_heads)\n",
    "    init_ma2,ma2=MultiHeadLayer(embed_dim,num_heads)\n",
    "    init_ffn,ffn=PointWiseFeedForwardNetwork(embed_dim,dff)\n",
    "    init_ln1,ln1=BatchNorm()\n",
    "    init_ln2,ln2=BatchNorm()\n",
    "    init_ln3,ln3=BatchNorm()\n",
    "    init_do1,do1=Dropout(rate)\n",
    "    init_do2,do2=Dropout(rate)\n",
    "    init_do3,do3=Dropout(rate)\n",
    "    def init_fun(rnd,input_shape):\n",
    "        rng_ma1,rng_ma2,rng_ffn,rng_ln1,rng_ln2,rng_ln3,rng_do1,rng_do2,rng_do3=random.split(rng,9)\n",
    "        shape, param_ma1   =init_ma1 (rng_ma1  ,input_shape)\n",
    "        shape, param_ma2   =init_ma2 (rng_ma2  ,input_shape)\n",
    "        shape, param_ffn   =init_ffn (rng_ffn  ,input_shape)\n",
    "        shape, param_ln1   =init_ln1 (rng_ln1  ,input_shape)\n",
    "        shape, param_ln2   =init_ln2 (rng_ln2  ,input_shape)\n",
    "        shape, param_ln3   =init_ln3 (rng_ln3  ,input_shape)\n",
    "        shape, param_do1   =init_do1 (rng_do1  ,input_shape)\n",
    "        shape, param_do2   =init_do2 (rng_do2  ,input_shape)\n",
    "        shape, param_do3   =init_do3 (rng_do3  ,input_shape)\n",
    "        return in_shape,(param_ma1,param_ma2,param_ffn,param_ln1,param_ln2,param_ln3,param_do1,param_do2,param_do3)\n",
    "    def apply_fun(params,x,enc_output,rng,training=True,mask_look_ahead=None,mask_padding=None):\n",
    "        param_ma1,param_ma2,param_ffn,param_ln1,param_ln2,param_ln3,param_do1,param_do2,param_do3=params\n",
    "        att1, att1_w=ma1(param_ma1,(x,x,x),mask_look_ahead)\n",
    "        att1        =do1(param_do1,att1,rng=rng,mode=training)\n",
    "        out1        =ln1(param_ln1,x+att1)\n",
    "        \n",
    "        att2, att2_w=ma2(param_ma2,(enc_output,enc_output,out1),mask_padding)\n",
    "        att2        =do2(param_do2,att2,rng=rng,mode=training)\n",
    "        out2        =ln2(param_ln2,out1+att2)\n",
    "        \n",
    "        ffn_output=ffn(param_ffn,out2)\n",
    "        ffn_output=do3(param_do3,ffn_output,rng=rng,mode=training)\n",
    "        out3=ln3(param_ln3,out2+ffn_output)\n",
    "        return out2, att1_w, att2_w\n",
    "    return init_fun,apply_fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "reported-ivory",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_shape= (64, 43, 512)\n",
      "o = (64, 43, 512)\n"
     ]
    }
   ],
   "source": [
    "in_shape = (64,43,512)\n",
    "init_dcl,dcl=DecoderLayer(512,8,2048)\n",
    "output_shape, params = init_dcl(rng, in_shape)\n",
    "fake_data=random.uniform(rng,in_shape)\n",
    "print(\"output_shape=\",output_shape)\n",
    "o,_,_=dcl(params,fake_data,oen,rng)\n",
    "print(\"o =\",o.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "ecological-sierra",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I don't know if this works fine\n",
    "# from: https://github.com/google/jax/pull/2157/files\n",
    "def Embedding(vocab_size,\n",
    "              embedding_size,\n",
    "              padding_idx=None,\n",
    "              embedding_init=uniform()):\n",
    "  \"\"\"Layer construction function for an embedding layer.\"\"\"\n",
    "\n",
    "  def init_fun(rng, input_shape):\n",
    "    embedding_shape = (vocab_size, embedding_size)\n",
    "    embedding_table = embedding_init(rng, embedding_shape)\n",
    "    if padding_idx is not None:\n",
    "      embedding_table = index_update(embedding_table, padding_idx, 0.)\n",
    "    output_shape = input_shape + (embedding_size,)\n",
    "    return output_shape, (embedding_table,)\n",
    "\n",
    "  def apply_fun(params, inputs, **kwargs):\n",
    "    embedding_table = params[0]\n",
    "    return embedding_table[inputs]\n",
    "\n",
    "  return init_fun, apply_fun\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "circular-kelly",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Encoder(num_layers,embed_dim,num_heads,dff,input_vocab_size,maximum_position_encoding,rate=0.1):\n",
    "    init_emb,emb=Embedding(input_vocab_size,embed_dim)\n",
    "    #init_pos,pos=PositionalEncoding(maximum_position_encoding,embed_dim)\n",
    "    enc_layers=[EncoderLayer(embed_dim,num_heads,dff,rate) for _ in range(num_layers)]\n",
    "    init_dou,dou=Dropout(rate)\n",
    "    def init_fun(rng,input_shape):\n",
    "        rng_emb,rng_pos,rng_dou=random.split(rng,3)\n",
    "        shape, param_emb=init_emb(rng_emb,input_shape)\n",
    "        #shape, param_pos=init_pos(rng_pos,input_shape)\n",
    "        param_pos=None\n",
    "        shape, param_dou=init_dou(rng_dou,input_shape)\n",
    "        params_enc=[]\n",
    "        for init_enc,_ in enc_layers:\n",
    "            rng_enc,rng=random.split(rng)\n",
    "            shape, param_enc=init_enc(rng_enc,input_shape)\n",
    "            params_enc.append(param_enc)\n",
    "        return input_shape,(param_emb,param_pos,params_enc,param_dou)\n",
    "    def apply_fun(params,x,rng,training,mask=None):\n",
    "        seq_len=x.shape[1]\n",
    "        param_emb,param_pos,params_enc,param_dou=params\n",
    "        x =emb(param_emb,x)\n",
    "        x*=np.sqrt(embed_dim)\n",
    "        #x+=pos(param_pos,x)#?????????????????????????????????\n",
    "        x=dou(param_dou,x,rng,mode=training)\n",
    "        for i,_,enc in enumerate(enc_layers):\n",
    "            x=enc(params_enc[i],x,training,mask)\n",
    "        return x\n",
    "    return init_fun,apply_fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "insured-tragedy",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "dtype argument to `uniform` must be a float dtype, got <class 'jax._src.numpy.lax_numpy.int64'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-206-77da2928cdbd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0minit_enc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0menc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2048\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0moutput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_enc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mfake_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrng\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0min_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"output_shape=\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfake_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrng\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dml3.8/lib/python3.8/site-packages/jax/_src/random.py\u001b[0m in \u001b[0;36muniform\u001b[0;34m(key, shape, dtype, minval, maxval)\u001b[0m\n\u001b[1;32m    368\u001b[0m   \"\"\"\n\u001b[1;32m    369\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missubdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloating\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m     raise ValueError(f\"dtype argument to `uniform` must be a float dtype, \"\n\u001b[0m\u001b[1;32m    371\u001b[0m                      f\"got {dtype}\")\n\u001b[1;32m    372\u001b[0m   \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanonicalize_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: dtype argument to `uniform` must be a float dtype, got <class 'jax._src.numpy.lax_numpy.int64'>"
     ]
    }
   ],
   "source": [
    "in_shape = (64,62)\n",
    "init_enc,enc=Encoder(2,512,8,2048,8500,10000)\n",
    "output_shape, params = init_enc(rng, in_shape)\n",
    "fake_data=random.uniform(rng,in_shape,dtype=np.int64)\n",
    "print(\"output_shape=\",output_shape)\n",
    "o,_,_=enc(params,fake_data,oen,rng)\n",
    "print(\"o =\",o.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "potential-invite",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
