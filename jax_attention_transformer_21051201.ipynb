{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "royal-count",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Francisco Dominguez Mateos\n",
    "# 09/05/2021\n",
    "# Inspired by: https://www.tensorflow.org/tutorials/text/transformer\n",
    "# Playing with attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "departmental-scotland",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from jax import random\n",
    "import jax.numpy as np\n",
    "from jax.experimental.stax import Dense, serial, Relu, BatchNorm, Dropout\n",
    "from jax.nn.initializers import uniform\n",
    "from jax.ops import index_update\n",
    "# Current convention is to import original numpy as \"onp\"\n",
    "import numpy as onp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "congressional-service",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate key which is used to generate random numbers\n",
    "rng = random.PRNGKey(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "looking-boxing",
   "metadata": {},
   "source": [
    "# Positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "polish-gauge",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from: https://www.tensorflow.org/tutorials/text/transformer\n",
    "def get_angles(pos, i, d_model):\n",
    "  angle_rates = 1 / onp.power(10000, (2 * (i//2)) / onp.float32(d_model))\n",
    "  return pos * angle_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bright-decline",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(position, d_model):\n",
    "  angle_rads = get_angles(onp.arange(position)[:, onp.newaxis],\n",
    "                          onp.arange(d_model)[onp.newaxis, :],\n",
    "                          d_model)\n",
    "\n",
    "  # apply sin to even indices in the array; 2i\n",
    "  angle_rads[:, 0::2] = onp.sin(angle_rads[:, 0::2])\n",
    "\n",
    "  # apply cos to odd indices in the array; 2i+1\n",
    "  angle_rads[:, 1::2] = onp.cos(angle_rads[:, 1::2])\n",
    "\n",
    "  pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "  return np.array(pos_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "joint-clock",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2048, 512)\n"
     ]
    }
   ],
   "source": [
    "n, d = 2048, 512\n",
    "pos_encoding = positional_encoding(n, d)\n",
    "print(pos_encoding.shape)\n",
    "pos_encoding = pos_encoding[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "harmful-hopkins",
   "metadata": {},
   "source": [
    "# Masking\n",
    "## Padding Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "polished-sampling",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "  seq =seq == 0\n",
    "  # add extra dimensions to add the padding\n",
    "  # to the attention logits.\n",
    "  return seq[:, newaxis, newaxis, :]  # (batch_size, 1, 1, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "decent-glass",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'newaxis' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-a8d67b4ad9a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcreate_padding_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-2b7b2f476086>\u001b[0m in \u001b[0;36mcreate_padding_mask\u001b[0;34m(seq)\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;31m# add extra dimensions to add the padding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;31m# to the attention logits.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewaxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewaxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# (batch_size, 1, 1, seq_len)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'newaxis' is not defined"
     ]
    }
   ],
   "source": [
    "x = np.array([[7, 6, 0, 0, 1], [1, 2, 3, 0, 0], [0, 0, 0, 4, 5]])\n",
    "create_padding_mask(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disturbed-globe",
   "metadata": {},
   "source": [
    "# Look Ahead Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "statewide-street",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(size):\n",
    "  mask = 1 - np.linalg.band_part(np.ones((size, size)), -1, 0)\n",
    "  return mask  # (seq_len, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "innovative-trigger",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'jax.numpy.linalg' has no attribute 'band_part'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-8e5d7789e43b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrng\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_look_ahead_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-8b1a2c23dab4>\u001b[0m in \u001b[0;36mcreate_look_ahead_mask\u001b[0;34m(size)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_look_ahead_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mband_part\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mmask\u001b[0m  \u001b[0;31m# (seq_len, seq_len)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'jax.numpy.linalg' has no attribute 'band_part'"
     ]
    }
   ],
   "source": [
    "x = random.uniform(rng,(1, 3))\n",
    "temp = create_look_ahead_mask(x.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "integral-evaluation",
   "metadata": {},
   "source": [
    "# Attention Scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "interim-surfing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(k,q,v):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    kT=np.swapaxes(k,-1,-2)\n",
    "    #print(\"q =\",q.shape)\n",
    "    #print(\"kT=\",kT.shape)\n",
    "    qk=np.matmul(q,kT)\n",
    "    #print(\"qk=\",qk.shape)\n",
    "    dk=k.shape[-1]\n",
    "    scalled_attention_logits=qk/np.sqrt(dk)\n",
    "    attention_weights=jax.nn.softmax(scalled_attention_logits)\n",
    "    #print(\"attention_weights=\",attention_weights.shape)\n",
    "    output=np.matmul(attention_weights,v)\n",
    "    return output, attention_weights\n",
    "def attention_scaled_dot_product(k,q,v,mask=None):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    kT=np.swapaxes(k,-1,-2)\n",
    "    #print(\"q =\",q.shape)\n",
    "    #print(\"kT=\",kT.shape)\n",
    "    qk=np.matmul(q,kT)\n",
    "    #print(\"qk=\",qk.shape)\n",
    "    dk=k.shape[-1]\n",
    "    scalled_attention_logits=qk/np.sqrt(dk)\n",
    "    # Apply the mask\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits+=(mask*-1e9)\n",
    "    attention_weights=jax.nn.softmax(scalled_attention_logits)\n",
    "    #print(\"attention_weights=\",attention_weights.shape)\n",
    "    output=np.matmul(attention_weights,v)\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "binding-commonwealth",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(formatter={'float': lambda x: \"{0:0.4f}\".format(x)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "rational-trail",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a= [[0.0000 1.0000 0.0000 0.0000]]\n",
      "o= [[10.0000 0.0000]]\n"
     ]
    }
   ],
   "source": [
    "temp_k = np.array([[10, 0, 0],\n",
    "                      [0, 10, 0],\n",
    "                      [0, 0, 10],\n",
    "                      [0, 0, 10]])  # (4, 3)\n",
    "\n",
    "temp_v = np.array([[1, 0],\n",
    "                      [10, 0],\n",
    "                      [100, 5],\n",
    "                      [1000, 6]])  # (4, 2)\n",
    "\n",
    "# This `query` aligns with the second `key`,\n",
    "# so the second `value` is returned.\n",
    "temp_q = np.array([[0, 10, 0]])  # (1, 3)\n",
    "o,a=attention_scaled_dot_product(temp_k,temp_q,temp_v)\n",
    "print(\"a=\",a)\n",
    "print(\"o=\",o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "original-graphic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a= [[0.0000 0.0000 0.5000 0.5000]]\n",
      "o= [[550.0000 5.5000]]\n"
     ]
    }
   ],
   "source": [
    "temp_q = np.array([[0, 0, 10]])  # (1, 3)\n",
    "o,a=attention_scaled_dot_product(temp_k,temp_q,temp_v)\n",
    "print(\"a=\",a)\n",
    "print(\"o=\",o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "elect-toyota",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a= [[0.5000 0.5000 0.0000 0.0000]]\n",
      "o= [[5.5000 0.0000]]\n"
     ]
    }
   ],
   "source": [
    "temp_q = np.array([[10, 10, 0]])  # (1, 3)\n",
    "o,a=attention_scaled_dot_product(temp_k,temp_q,temp_v)\n",
    "print(\"a=\",a)\n",
    "print(\"o=\",o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "humanitarian-depth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "[[0.0000 0.0000 0.5000 0.5000]\n",
      " [0.0000 1.0000 0.0000 0.0000]\n",
      " [0.5000 0.5000 0.0000 0.0000]]\n",
      "o=\n",
      "[[550.0000 5.5000]\n",
      " [10.0000 0.0000]\n",
      " [5.5000 0.0000]]\n"
     ]
    }
   ],
   "source": [
    "temp_q = np.array([[0, 0, 10],\n",
    "                      [0, 10, 0],\n",
    "                      [10, 10, 0]])  # (3, 3)\n",
    "o,a=attention_scaled_dot_product(temp_k,temp_q,temp_v)\n",
    "print(\"a=\")\n",
    "print(a)\n",
    "print(\"o=\")\n",
    "print(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minus-international",
   "metadata": {},
   "source": [
    "# Multi Head Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cathedral-nicaragua",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MultiHeadLayer(embed_dim, num_heads=8):\n",
    "    assert embed_dim % num_heads==0\n",
    "    depth=embed_dim//num_heads\n",
    "    init_wk ,wk =Dense(embed_dim)\n",
    "    init_wq ,wq =Dense(embed_dim)\n",
    "    init_wv ,wv =Dense(embed_dim)\n",
    "    init_lin,lin=Dense(embed_dim)\n",
    "    \n",
    "    def init_fun(rng,input_shape):\n",
    "        rng_k,rng_q,rng_v,rng_lin=random.split(rng,4)\n",
    "        shape, param_k   =init_wk (rng_k  ,input_shape)\n",
    "        shape, param_q   =init_wq (rng_q  ,input_shape)\n",
    "        shape, param_v   =init_wv (rng_v  ,input_shape)\n",
    "        shape, param_lin =init_lin(rng_lin,input_shape)\n",
    "        return shape,(param_k,param_q,param_v,param_lin)\n",
    "\n",
    "    def split_heads(x,batch_size):\n",
    "        x=np.reshape(x,(batch_size,-1,num_heads,depth))\n",
    "        x=np.transpose(x,(0,2,1,3))\n",
    "        return x\n",
    "    \n",
    "    def apply_fun(params,x,mask=None):\n",
    "        batch_size=x[0].shape[0]\n",
    "        k,q,v=x\n",
    "        param_k,param_q,param_v,param_lin=params\n",
    "        k=wk(param_k,k)\n",
    "        q=wq(param_q,q)\n",
    "        v=wv(param_v,v)\n",
    "        #print(\"k=\",k.shape)\n",
    "        #print(\"q=\",q.shape)\n",
    "        #print(\"v=\",v.shape)\n",
    "        k=split_heads(k,batch_size)\n",
    "        q=split_heads(q,batch_size)\n",
    "        v=split_heads(v,batch_size)\n",
    "        #print(\"k=\",k.shape)\n",
    "        #print(\"q=\",q.shape)\n",
    "        #print(\"v=\",v.shape)\n",
    "        a,aw=attention_scaled_dot_product(k,q,v)\n",
    "        sa=np.transpose(a,(0,2,1,3))\n",
    "        ca=np.reshape(sa,(batch_size,-1,embed_dim))\n",
    "        output=lin(param_lin,ca)\n",
    "        return output,aw\n",
    "    \n",
    "    return init_fun,apply_fun\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "married-hypothetical",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_shape= (1, 60, 512)\n",
      "o = (1, 60, 512)\n",
      "aw= (1, 8, 60, 60)\n"
     ]
    }
   ],
   "source": [
    "in_shape = (1,60,512)\n",
    "init_mha,mha=MultiHeadLayer(in_shape[2])\n",
    "output_shape, params = init_mha(rng, in_shape)\n",
    "fake_data=random.uniform(rng,in_shape)\n",
    "print(\"output_shape=\",output_shape)\n",
    "o,aw=mha(params,(fake_data,fake_data,fake_data))\n",
    "print(\"o =\",o.shape)\n",
    "print(\"aw=\",aw.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "proprietary-watershed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp_a= (2, 4, 3)\n",
      "[[[0.0000 0.0000 1.0000]\n",
      "  [0.0000 1.0000 0.0000]\n",
      "  [0.0000 1.0000 0.0000]\n",
      "  [1.0000 1.0000 0.0000]]\n",
      "\n",
      " [[0.0000 0.0000 -1.0000]\n",
      "  [0.0000 -1.0000 0.0000]\n",
      "  [0.0000 -1.0000 0.0000]\n",
      "  [-1.0000 -1.0000 0.0000]]]\n",
      "[[[-0.5774 -1.7321 1.7321]\n",
      "  [-0.5774 0.5774 -0.5774]\n",
      "  [-0.5774 0.5774 -0.5774]\n",
      "  [1.7321 0.5774 -0.5774]]\n",
      "\n",
      " [[-0.5773 -1.7056 1.7320]\n",
      "  [-0.5773 0.4264 -0.5773]\n",
      "  [-0.5773 0.8528 -0.5773]\n",
      "  [1.7320 0.4264 -0.5773]]]\n",
      "[[[-0.7071 -0.7071 1.4142]\n",
      "  [-0.7071 1.4142 -0.7071]\n",
      "  [-0.7071 1.4142 -0.7071]\n",
      "  [0.7071 0.7071 -1.4142]]\n",
      "\n",
      " [[-0.7071 -0.7071 1.4142]\n",
      "  [-0.7071 1.4142 -0.7071]\n",
      "  [-0.7071 1.4142 -0.7071]\n",
      "  [0.7071 0.7071 -1.4142]]]\n",
      "[[[-0.8452 -0.8452 1.1832]\n",
      "  [-0.8452 1.1832 -0.8452]\n",
      "  [-0.8452 1.1832 -0.8452]\n",
      "  [1.1832 1.1832 -0.8452]]\n",
      "\n",
      " [[-0.8409 -0.8409 1.0996]\n",
      "  [-0.8409 1.0996 -0.8409]\n",
      "  [-0.8409 1.4877 -0.8409]\n",
      "  [1.0996 1.0996 -0.8409]]]\n"
     ]
    }
   ],
   "source": [
    "#Normalizing ..... to implement layer normalization\n",
    "temp_a = np.array([[[0, 0, 10],\n",
    "                   [0, 10, 0],\n",
    "                   [0, 10, 0],\n",
    "                   [10, 10, 0]],\n",
    "                  [[0, 0, 5],\n",
    "                   [0, 5, 0],\n",
    "                   [0, 6, 0],\n",
    "                   [5, 5, 0]]])\n",
    "print(\"temp_a=\",temp_a.shape)\n",
    "print(jax.nn.normalize(temp_a,axis=0))\n",
    "print(jax.nn.normalize(temp_a,axis=1))\n",
    "print(jax.nn.normalize(temp_a,axis=2))\n",
    "print(jax.nn.normalize(temp_a,axis=(1,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sound-wrong",
   "metadata": {},
   "source": [
    "# Pointwise Feedforward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "caring-template",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PointWiseFeedForwardNetwork(embeb_dim,dff):\n",
    "    return serial(Dense(dff),Relu,Dense(embeb_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "chubby-soldier",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_shape= (64, 50, 512)\n",
      "o = (64, 50, 512)\n"
     ]
    }
   ],
   "source": [
    "in_shape = (64,50,512)\n",
    "init_dff,dff=PointWiseFeedForwardNetwork(512,2048)\n",
    "output_shape, params = init_dff(rng, in_shape)\n",
    "fake_data=random.uniform(rng,in_shape)\n",
    "print(\"output_shape=\",output_shape)\n",
    "o=dff(params,fake_data)\n",
    "print(\"o =\",o.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "western-threat",
   "metadata": {},
   "source": [
    "# Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "wooden-commission",
   "metadata": {},
   "outputs": [],
   "source": [
    "def EncoderLayer(embed_dim,num_heads,dff,rate=0.1):\n",
    "    init_mha,mha=MultiHeadLayer(embed_dim,num_heads)\n",
    "    init_ffn,ffn=PointWiseFeedForwardNetwork(embed_dim,dff)\n",
    "    init_ln1,ln1=BatchNorm()\n",
    "    init_ln2,ln2=BatchNorm()\n",
    "    init_do1,do1=Dropout(rate)\n",
    "    init_do2,do2=Dropout(rate)\n",
    "    def init_fun(rnd,input_shape):\n",
    "        rng_mha,rng_ffn,rng_ln1,rng_ln2,rng_do1,rng_do2=random.split(rng,6)\n",
    "        shape, param_mha   =init_mha (rng_mha  ,input_shape)\n",
    "        shape, param_ffn   =init_ffn (rng_ffn  ,input_shape)\n",
    "        shape, param_ln1   =init_ln1 (rng_ln1  ,input_shape)\n",
    "        shape, param_ln2   =init_ln2 (rng_ln2  ,input_shape)\n",
    "        shape, param_do1   =init_do1 (rng_do1  ,input_shape)\n",
    "        shape, param_do2   =init_do2 (rng_do2  ,input_shape)\n",
    "        return in_shape,(param_mha,param_ffn,param_ln1,param_ln2,param_do1,param_do2)\n",
    "    def apply_fun(params,x,rng,training=True,mask=None):\n",
    "        param_mha,param_ffn,param_ln1,param_ln2,param_do1,param_do2=params\n",
    "        attn_output, _=mha(param_mha,(x,x,x),mask)\n",
    "        attn_output=do1(param_do1,attn_output,rng=rng,mode=training)\n",
    "        #print(\"x=\",x.shape)\n",
    "        #print(\"attn_output\",attn_output.shape)\n",
    "        out1=ln1(param_ln1,x+attn_output)\n",
    "        \n",
    "        ffn_output=ffn(param_ffn,out1)\n",
    "        ffn_output=do2(param_do2,ffn_output,rng=rng,mode=training)\n",
    "        out2=ln2(param_ln2,out1+ffn_output)\n",
    "        return out2\n",
    "    return init_fun,apply_fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "secret-frontier",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_shape= (64, 43, 512)\n",
      "oel= (64, 43, 512)\n"
     ]
    }
   ],
   "source": [
    "in_shape = (64,43,512)\n",
    "init_enl,enl=EncoderLayer(512,8,2048)\n",
    "output_shape, params = init_enl(rng, in_shape)\n",
    "fake_data=random.uniform(rng,in_shape)\n",
    "print(\"output_shape=\",output_shape)\n",
    "oel=enl(params,fake_data,rng)\n",
    "print(\"oel=\",oel.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accepted-motion",
   "metadata": {},
   "source": [
    "# Decoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "smaller-shadow",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DecoderLayer(embed_dim,num_heads,dff,rate=0.1):\n",
    "    init_ma1,ma1=MultiHeadLayer(embed_dim,num_heads)\n",
    "    init_ma2,ma2=MultiHeadLayer(embed_dim,num_heads)\n",
    "    init_ffn,ffn=PointWiseFeedForwardNetwork(embed_dim,dff)\n",
    "    init_ln1,ln1=BatchNorm()\n",
    "    init_ln2,ln2=BatchNorm()\n",
    "    init_ln3,ln3=BatchNorm()\n",
    "    init_do1,do1=Dropout(rate)\n",
    "    init_do2,do2=Dropout(rate)\n",
    "    init_do3,do3=Dropout(rate)\n",
    "    def init_fun(rnd,input_shape):\n",
    "        rng_ma1,rng_ma2,rng_ffn,rng_ln1,rng_ln2,rng_ln3,rng_do1,rng_do2,rng_do3=random.split(rng,9)\n",
    "        shape, param_ma1   =init_ma1 (rng_ma1  ,input_shape)\n",
    "        shape, param_ma2   =init_ma2 (rng_ma2  ,input_shape)\n",
    "        shape, param_ffn   =init_ffn (rng_ffn  ,input_shape)\n",
    "        shape, param_ln1   =init_ln1 (rng_ln1  ,input_shape)\n",
    "        shape, param_ln2   =init_ln2 (rng_ln2  ,input_shape)\n",
    "        shape, param_ln3   =init_ln3 (rng_ln3  ,input_shape)\n",
    "        shape, param_do1   =init_do1 (rng_do1  ,input_shape)\n",
    "        shape, param_do2   =init_do2 (rng_do2  ,input_shape)\n",
    "        shape, param_do3   =init_do3 (rng_do3  ,input_shape)\n",
    "        return input_shape,(param_ma1,param_ma2,param_ffn,param_ln1,param_ln2,param_ln3,param_do1,param_do2,param_do3)\n",
    "    def apply_fun(params,x,enc_output,rng,training=True,mask_look_ahead=None,mask_padding=None):\n",
    "        param_ma1,param_ma2,param_ffn,param_ln1,param_ln2,param_ln3,param_do1,param_do2,param_do3=params\n",
    "        att1, att1_w=ma1(param_ma1,(x,x,x),mask_look_ahead)\n",
    "        att1        =do1(param_do1,att1,rng=rng,mode=training)\n",
    "        out1        =ln1(param_ln1,x+att1)\n",
    "        \n",
    "        att2, att2_w=ma2(param_ma2,(enc_output,out1,enc_output),mask_padding)\n",
    "        att2        =do2(param_do2,att2,rng=rng,mode=training)\n",
    "        out2        =ln2(param_ln2,out1+att2)\n",
    "        \n",
    "        ffn_output=ffn(param_ffn,out2)\n",
    "        ffn_output=do3(param_do3,ffn_output,rng=rng,mode=training)\n",
    "        out3=ln3(param_ln3,out2+ffn_output)\n",
    "        return out2, att1_w, att2_w\n",
    "    return init_fun,apply_fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "reported-ivory",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_shape= (64, 43, 512)\n",
      "o = (64, 43, 512)\n"
     ]
    }
   ],
   "source": [
    "in_shape = (64,43,512)\n",
    "init_dcl,dcl=DecoderLayer(512,8,2048)\n",
    "output_shape, params = init_dcl(rng, in_shape)\n",
    "fake_data=random.uniform(rng,in_shape)\n",
    "print(\"output_shape=\",output_shape)\n",
    "o,_,_=dcl(params,fake_data,oel,rng)\n",
    "print(\"o =\",o.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demographic-serve",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ecological-sierra",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I don't know if this works fine\n",
    "# from: https://github.com/google/jax/pull/2157/files\n",
    "def Embedding(vocab_size,\n",
    "              embedding_size,\n",
    "              padding_idx=None,\n",
    "              embedding_init=uniform()):\n",
    "  \"\"\"Layer construction function for an embedding layer.\"\"\"\n",
    "  def init_fun(rng, input_shape):\n",
    "    embedding_shape = (vocab_size, embedding_size)\n",
    "    embedding_table = embedding_init(rng, embedding_shape)\n",
    "    if padding_idx is not None:\n",
    "      embedding_table = index_update(embedding_table, padding_idx, 0.)\n",
    "    output_shape = input_shape + (embedding_size,)\n",
    "    return output_shape, (embedding_table,)\n",
    "\n",
    "  def apply_fun(params, inputs):\n",
    "    embedding_table = params[0]\n",
    "    return embedding_table[inputs]\n",
    "\n",
    "  return init_fun, apply_fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "authorized-disposal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params= (DeviceArray([[0.0016, 0.0016],\n",
      "             [0.0041, 0.0009],\n",
      "             [0.0077, 0.0002],\n",
      "             [0.0025, 0.0047],\n",
      "             [0.0042, 0.0036],\n",
      "             [0.0008, 0.0003],\n",
      "             [0.0095, 0.0060],\n",
      "             [0.0004, 0.0024],\n",
      "             [0.0081, 0.0042],\n",
      "             [0.0070, 0.0030]], dtype=float32),)\n",
      "fake_data= [[4 2 3 2 4 2 2 3]\n",
      " [5 4 3 5 0 3 2 7]]\n",
      "output_shape= (2, 8, 2)\n",
      "o = (2, 8, 2)\n",
      "[[[0.0042 0.0036]\n",
      "  [0.0077 0.0002]\n",
      "  [0.0025 0.0047]\n",
      "  [0.0077 0.0002]\n",
      "  [0.0042 0.0036]\n",
      "  [0.0077 0.0002]\n",
      "  [0.0077 0.0002]\n",
      "  [0.0025 0.0047]]\n",
      "\n",
      " [[0.0008 0.0003]\n",
      "  [0.0042 0.0036]\n",
      "  [0.0025 0.0047]\n",
      "  [0.0008 0.0003]\n",
      "  [0.0016 0.0016]\n",
      "  [0.0025 0.0047]\n",
      "  [0.0077 0.0002]\n",
      "  [0.0004 0.0024]]]\n"
     ]
    }
   ],
   "source": [
    "in_shape = (2,8)\n",
    "init_emb,emb=Embedding(10,2)\n",
    "output_shape, params = init_emb(rng, in_shape)\n",
    "print(\"params=\",params)\n",
    "fake_data=random.randint(rng,in_shape,minval=0,maxval=10)\n",
    "print(\"fake_data=\",fake_data)\n",
    "print(\"output_shape=\",output_shape)\n",
    "o=emb(params,fake_data)\n",
    "print(\"o =\",o.shape)\n",
    "print(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contained-islam",
   "metadata": {},
   "source": [
    "# Encoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "circular-kelly",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Encoder(num_layers,embed_dim,num_heads,dff,input_vocab_size,maximum_position_encoding,rate=0.1):\n",
    "    init_emb,emb=Embedding(input_vocab_size,embed_dim)\n",
    "    pos_encoding=positional_encoding(maximum_position_encoding,embed_dim)\n",
    "    enc_layers=[EncoderLayer(embed_dim,num_heads,dff,rate) for _ in range(num_layers)]\n",
    "    init_dou,dou=Dropout(rate)\n",
    "    def init_fun(rng,input_shape):\n",
    "        rng_emb,rng_pos,rng_dou=random.split(rng,3)\n",
    "        out_shape, param_emb=init_emb(rng_emb,input_shape)\n",
    "        shape, param_dou=init_dou(rng_dou,out_shape)\n",
    "        params_enc=[]\n",
    "        for init_enc,_ in enc_layers:\n",
    "            rng_enc,rng=random.split(rng)\n",
    "            shape, param_enc=init_enc(rng_enc,out_shape)\n",
    "            params_enc.append(param_enc)\n",
    "        return out_shape,(param_emb,params_enc,param_dou)\n",
    "    def apply_fun(params,x,rng,training=True,mask=None):\n",
    "        param_emb,params_enc,param_dou=params\n",
    "        seq_len=x.shape[1]\n",
    "        x =emb(param_emb,x)\n",
    "        x*=np.sqrt(embed_dim)\n",
    "        x+=pos_encoding[:, :seq_len, :]\n",
    "        x=dou(param_dou,x,rng=rng,mode=training)\n",
    "        for i,layer in enumerate(enc_layers):\n",
    "            _,enc=layer\n",
    "            x=enc(params_enc[i],x,rng,training,mask)\n",
    "        return x\n",
    "    return init_fun,apply_fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "insured-tragedy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_shape= (64, 62, 512)\n",
      "oen = (64, 62, 512)\n"
     ]
    }
   ],
   "source": [
    "in_shape = (64,62)\n",
    "init_enc,enc=Encoder(2,512,8,2048,8500,10000)\n",
    "output_shape, params = init_enc(rng, in_shape)\n",
    "fake_data=random.randint(rng,in_shape,minval=0,maxval=200)\n",
    "print(\"output_shape=\",output_shape)\n",
    "oen=enc(params,fake_data,rng)\n",
    "print(\"oen =\",oen.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "potential-invite",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Decoder(num_layers,embed_dim,num_heads,dff,target_vocab_size,maximum_position_encoding,rate=0.1):\n",
    "    init_emb,emb=Embedding(target_vocab_size,embed_dim)\n",
    "    pos_encoding=positional_encoding(maximum_position_encoding,embed_dim)\n",
    "    dec_layers=[DecoderLayer(embed_dim,num_heads,dff,rate) for _ in range(num_layers)]\n",
    "    init_dou,dou=Dropout(rate)\n",
    "    def init_fun(rng,input_shape):\n",
    "        rng_emb,rng_dou,rng=random.split(rng,3)\n",
    "        out_shape, param_emb=init_emb(rng_emb,input_shape)\n",
    "        shape, param_dou=init_dou(rng_dou,out_shape)\n",
    "        params_dec=[]\n",
    "        for init_dec,_ in dec_layers:\n",
    "            rng_dec,rng=random.split(rng)\n",
    "            shape, param_dec=init_dec(rng_dec,out_shape)\n",
    "            params_dec.append(param_dec)\n",
    "        return out_shape,(param_emb,params_dec,param_dou)\n",
    "    def apply_fun(params,x,enc_output,rng,training=True,mask_look_ahead=None,mask_padding=None):\n",
    "        param_emb,params_dec,param_dou=params\n",
    "        seq_len=x.shape[1]\n",
    "        attn_w={}\n",
    "        x =emb(param_emb,x)\n",
    "        x*=np.sqrt(embed_dim)\n",
    "        x+=pos_encoding[:, :seq_len, :]\n",
    "        x=dou(param_dou,x,rng=rng,mode=training)\n",
    "        for i,layer in enumerate(dec_layers):\n",
    "            _,dec=layer\n",
    "            x, att1_w,att2_w=dec(params_dec[i],x,enc_output,rng,training,mask_look_ahead,mask_padding)\n",
    "            attn_w[f'decoder_layer{i+1}_block1']=att1_w\n",
    "            attn_w[f'decoder_layer{i+1}_block2']=att2_w\n",
    "        return x,attn_w \n",
    "    return init_fun,apply_fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "virtual-acoustic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_shape= (64, 26, 512)\n",
      "odc = (64, 26, 512)\n",
      "attn_w= (64, 8, 26, 62)\n"
     ]
    }
   ],
   "source": [
    "in_shape = (64,26)\n",
    "init_dec,dec=Decoder(2,512,8,2048,8000,5000)\n",
    "output_shape, params = init_dec(rng, in_shape)\n",
    "fake_data=random.randint(rng,in_shape,minval=0,maxval=200)\n",
    "print(\"output_shape=\",output_shape)\n",
    "odc,attn_w=dec(params,fake_data,oen,rng)\n",
    "print(\"odc =\",odc.shape)\n",
    "print(\"attn_w=\",attn_w['decoder_layer2_block2'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "rational-conservation",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Transformer(num_layers, embed_dim, num_heads, dff, input_vocab_size,\n",
    "               target_vocab_size, pe_input, pe_target, rate=0.1):\n",
    "    init_enc,enc=Encoder(num_layers,embed_dim,num_heads,dff,target_vocab_size,pe_input ,rate=0.1)\n",
    "    init_dec,dec=Decoder(num_layers,embed_dim,num_heads,dff,target_vocab_size,pe_target,rate=0.1)\n",
    "    init_fnl,fnl=Dense(target_vocab_size)\n",
    "    def init_fun(rng,input_enc_shape,input_dec_shape):\n",
    "        rng_enc,rng_dec,rng_fnl=random.split(rng,3)\n",
    "        output_enc_shape, param_enc=init_enc(rng_enc,input_enc_shape)\n",
    "        output_dec_shape, param_dec=init_dec(rng_dec,input_dec_shape)\n",
    "        output_shape    , param_fnl=init_fnl(rng_fnl,output_dec_shape)\n",
    "        return output_shape,(param_enc,param_dec,param_fnl)\n",
    "    def apply_fun(params,enc_x,dec_x,rng,training=True,mask_enc_padding=None,mask_look_ahead=None,mask_dec_padding=None):\n",
    "        param_enc,param_dec,param_fnl=params\n",
    "        enc_output       =enc(param_enc,enc_x,rng,training,mask_enc_padding)\n",
    "        dec_output,attn_w=dec(param_dec,dec_x,enc_output,rng,training,mask_look_ahead,mask_dec_padding)\n",
    "        final_output     =fnl(param_fnl,dec_output)\n",
    "        return final_output,attn_w\n",
    "    return init_fun,apply_fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "liable-shipping",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_shape= (64, 36, 8000)\n",
      "otr = (64, 36, 8000)\n",
      "attn_w= (64, 8, 36, 38)\n"
     ]
    }
   ],
   "source": [
    "in_enc_shape = (64,38)\n",
    "in_dec_shape = (64,36)\n",
    "init_trn,trn=Transformer(2,512,8,2048,8500,8000,10000,6000)\n",
    "output_shape, params = init_trn(rng, in_enc_shape,in_dec_shape)\n",
    "fake_enc_data=random.randint(rng,in_enc_shape,minval=0,maxval=200)\n",
    "fake_dec_data=random.randint(rng,in_dec_shape,minval=0,maxval=200)\n",
    "print(\"output_shape=\",output_shape)\n",
    "otr,attn_w=trn(params,fake_enc_data,fake_dec_data,rng)\n",
    "print(\"otr =\",otr.shape)\n",
    "print(\"attn_w=\",attn_w['decoder_layer2_block2'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empirical-latin",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
