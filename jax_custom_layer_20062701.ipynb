{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Francisco Dominguez Mateos\n",
    "# 27/06/2020\n",
    "# NOTHING working in here\n",
    "# Defining a custom layer\n",
    "#!conda install -c conda-forge opencv --yes\n",
    "#!conda install -c anaconda tensorflow-gpu --yes\n",
    "#!conda install -c conda-forge tensorflow-probability --yes\n",
    "#!conda install -c conda-forge tensorboard --yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as onp\n",
    "import jax.numpy as np\n",
    "from jax import grad, jit, vmap, value_and_grad\n",
    "from jax import random\n",
    "from jax.experimental import stax\n",
    "from jax.experimental.stax import BatchNorm, Dense, Relu, LogSoftmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate key which is used to generate random numbers\n",
    "rng = random.PRNGKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes=10\n",
    "init_mlp, mlp = stax.serial(Dense(128),\n",
    "                            #BatchNorm(), #doesn't seem to work with Dense!!!\n",
    "                            Relu,\n",
    "                            Dense(128),\n",
    "                            #BatchNorm(),\n",
    "                            Relu,\n",
    "                            Dense(num_classes),\n",
    "                            LogSoftmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 21, 784)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(100, 21, 10)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Very interesting if we put another dimension 20 in this case\n",
    "# the model is repeated 20 times and output 20 times num_classes\n",
    "in_shape = (-1, 20, 28*28)\n",
    "_, params = init_mlp(rng, in_shape)\n",
    "fake_data=random.uniform(rng,(100,21,28*28))\n",
    "print(fake_data.shape)\n",
    "mlp(params,fake_data).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Siren(sizes=[1000]*5):\n",
    "    def init_func(key,input_shape):\n",
    "        \"\"\" Initialize the weights of all layers of a linear layer network \"\"\"\n",
    "        keys = random.split(key, len(sizes))\n",
    "        # Initialize a single layer with Gaussian weights -  helper function\n",
    "        def initialize_layer(m, n, rng, scale=1e-2):\n",
    "            w_key, b_key = random.split(key)\n",
    "            return scale * random.normal(w_key, (n, m)), scale * random.normal(b_key, (n,))\n",
    "        return [initialize_layer(m, n, k) for m, n, k in zip(sizes[:-1], sizes[1:], key)]\n",
    "    def apply_fun(params, inputs, **kwargs):\n",
    "        \"\"\" Compute the forward pass for each example individually \"\"\"\n",
    "        activations = inputs\n",
    "        # Loop over the Siren hidden layers\n",
    "        for w, b in params:\n",
    "            linear = np.dot(w,activations)+b\n",
    "            activations=np.sin(linear)\n",
    "        return activations        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GRU(out_dim, W_init=glorot_normal(), b_init=normal()):\n",
    "    def init_fun(rng, input_shape):\n",
    "        \"\"\" Initialize the GRU layer for stax \"\"\"\n",
    "        hidden = b_init(rng, (input_shape[0], out_dim))\n",
    "\n",
    "        k1, k2, k3 = random.split(rng, num=3)\n",
    "        update_W, update_U, update_b = (\n",
    "            W_init(k1, (input_shape[2], out_dim)),\n",
    "            W_init(k2, (out_dim, out_dim)),\n",
    "            b_init(k3, (out_dim,)),)\n",
    "\n",
    "        k1, k2, k3 = random.split(rng, num=3)\n",
    "        reset_W, reset_U, reset_b = (\n",
    "            W_init(k1, (input_shape[2], out_dim)),\n",
    "            W_init(k2, (out_dim, out_dim)),\n",
    "            b_init(k3, (out_dim,)),)\n",
    "\n",
    "        k1, k2, k3 = random.split(rng, num=3)\n",
    "        out_W, out_U, out_b = (\n",
    "            W_init(k1, (input_shape[2], out_dim)),\n",
    "            W_init(k2, (out_dim, out_dim)),\n",
    "            b_init(k3, (out_dim,)),)\n",
    "        # Input dim 0 represents the batch dimension\n",
    "        # Input dim 1 represents the time dimension (before scan moveaxis)\n",
    "        output_shape = (input_shape[0], input_shape[1], out_dim)\n",
    "        return (output_shape,\n",
    "            (hidden,\n",
    "             (update_W, update_U, update_b),\n",
    "             (reset_W, reset_U, reset_b),\n",
    "             (out_W, out_U, out_b),),)\n",
    "\n",
    "    def apply_fun(params, inputs, **kwargs):\n",
    "        \"\"\" Loop over the time steps of the input sequence \"\"\"\n",
    "        h = params[0]\n",
    "\n",
    "        def apply_fun_scan(params, hidden, inp):\n",
    "            \"\"\" Perform single step update of the network \"\"\"\n",
    "            _, (update_W, update_U, update_b), (reset_W, reset_U, reset_b), (\n",
    "                out_W, out_U, out_b) = params\n",
    "\n",
    "            update_gate = sigmoid(np.dot(inp, update_W) +\n",
    "                                  np.dot(hidden, update_U) + update_b)\n",
    "            reset_gate = sigmoid(np.dot(inp, reset_W) +\n",
    "                                 np.dot(hidden, reset_U) + reset_b)\n",
    "            output_gate = np.tanh(np.dot(inp, out_W)\n",
    "                                  + np.dot(np.multiply(reset_gate, hidden), out_U)\n",
    "                                  + out_b)\n",
    "            output = np.multiply(update_gate, hidden) + np.multiply(1-update_gate, output_gate)\n",
    "            hidden = output\n",
    "            return hidden, hidden\n",
    "\n",
    "        # Move the time dimension to position 0\n",
    "        inputs = np.moveaxis(inputs, 1, 0)\n",
    "        f = partial(apply_fun_scan, params)\n",
    "        _, h_new = lax.scan(f, h, inputs)\n",
    "        return h_new\n",
    "\n",
    "    return init_fun, apply_fun"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
