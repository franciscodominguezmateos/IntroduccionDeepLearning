{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "absent-experiment",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "moved-springfield",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_base=\"/home/francisco/datasets/datasets/sound/voice/stress\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "talented-generic",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLabel(s):\n",
    "    if s==\"TRUE\":\n",
    "        return 1.0\n",
    "    if s==\"FALSE\":\n",
    "        return 0.0\n",
    "    if s==\"PC\":\n",
    "        return 2.0\n",
    "def isRightLabel(s):\n",
    "    if s==\"TRUE\":\n",
    "        return True\n",
    "    if s==\"FALSE\":\n",
    "        return True\n",
    "    if s==\"PC\":\n",
    "        return False\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "velvet-attendance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: /home/francisco/datasets/datasets/sound/voice/stress/Set_Males.csv\n",
      "Processing file: /home/francisco/datasets/datasets/sound/voice/stress/Set_Females.csv\n",
      "(50, 68)\n",
      "[[1.26278e+02 1.40000e-02 2.90000e-02 ... 1.56810e+01 8.70000e-01\n",
      "  1.20000e-02]\n",
      " [1.42901e+02 5.00000e-03 1.90000e-02 ... 1.08340e+01 9.36000e-01\n",
      "  8.00000e-03]\n",
      " [1.17189e+02 5.00000e-03 1.40000e-02 ... 1.17650e+01 9.67000e-01\n",
      "  6.00000e-03]\n",
      " ...\n",
      " [1.92269e+02 8.00000e-03 2.20000e-02 ... 1.06000e+01 9.28000e-01\n",
      "  7.00000e-03]\n",
      " [1.67700e+02 5.00000e-03 2.50000e-02 ... 9.56400e+00 9.33000e-01\n",
      "  9.00000e-03]\n",
      " [2.06086e+02 7.00000e-03 1.30000e-02 ... 9.85100e+00 9.22000e-01\n",
      "  4.60000e-02]]\n",
      "(50,)\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "data=[]\n",
    "labels=[]\n",
    "file_txt=path_base+\"/*.csv\"\n",
    "for filepath in glob.glob(file_txt):\n",
    "    print(\"Processing file: {}\".format(filepath)) \n",
    "    with open(filepath) as fp:  \n",
    "        line = fp.readline()\n",
    "        head_list=line.split(\";\")\n",
    "        #for i,head in enumerate(head_list):\n",
    "        #    print(i,head)\n",
    "        cnt = 1\n",
    "        line = fp.readline()\n",
    "        while line:\n",
    "            data_row=[]\n",
    "            line_list=line.split(\";\")\n",
    "            #print(\"Line {}: {} {} {}\".format(cnt, line_list[0], line_list[1], line_list[2]))\n",
    "            line = fp.readline()\n",
    "            cnt += 1\n",
    "            if not isRightLabel(line_list[0]):\n",
    "                continue\n",
    "            for i,datum in enumerate(line_list):\n",
    "                #print(i,head_list[i],\"=\",datum)\n",
    "                if i>1:\n",
    "                    data_row.append(float(datum))\n",
    "                if i==0:\n",
    "                    labels.append(getLabel(datum))\n",
    "            data.append(data_row)\n",
    "data_np=np.array(data)\n",
    "labels_np=np.array(labels)\n",
    "print(data_np.shape)\n",
    "print(data_np)\n",
    "print(labels_np.shape)\n",
    "print(labels_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "double-massage",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "colored-isaac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 68)\n",
      "[[-0.69164371  1.33540079  0.94208383 ...  1.05566522 -1.54961754\n",
      "  -0.52359428]\n",
      " [-0.30927853 -0.90687665 -0.1000443  ... -0.26111133  0.01087617\n",
      "  -0.80815639]\n",
      " [-0.90071049 -0.90687665 -0.62110836 ... -0.00818808  0.74383534\n",
      "  -0.95043744]\n",
      " ...\n",
      " [ 0.82629293 -0.15945084  0.21259414 ... -0.32468173 -0.17827458\n",
      "  -0.87929692]\n",
      " [ 0.26115245 -0.90687665  0.52523258 ... -0.60613015 -0.06005536\n",
      "  -0.73701586]\n",
      " [ 1.14411401 -0.40859278 -0.72532118 ... -0.52816133 -0.32013765\n",
      "   1.89518365]]\n"
     ]
    }
   ],
   "source": [
    "ss=StandardScaler()\n",
    "data_ss=ss.fit_transform(data_np)\n",
    "\n",
    "print(data_ss.shape)\n",
    "print(data_ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "upper-interstate",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as onp\n",
    "import jax.numpy as np\n",
    "from jax import grad, jit, vmap, value_and_grad\n",
    "from jax import random\n",
    "from jax.experimental import stax\n",
    "from jax.experimental.stax import (BatchNorm, Conv, Dense, Flatten, Dropout,\n",
    "                                   Relu, LogSoftmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "closed-promise",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some additional JAX and dataloader helpers\n",
    "from jax.scipy.special import logsumexp\n",
    "from jax.experimental import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "known-leader",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu\n"
     ]
    }
   ],
   "source": [
    "#Test if JAX is using CPU or GPU\n",
    "from jax.lib import xla_bridge\n",
    "print(xla_bridge.get_backend().platform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "falling-attribute",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate key which is used to generate random numbers\n",
    "key = random.PRNGKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "identified-guatemala",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "hidden=25\n",
    "dropout_rate=0.5\n",
    "# Conv seems to work with NHWC but it works with NCHW too\n",
    "init_fun, net = stax.serial(Dense(hidden),\n",
    "                            Dropout(dropout_rate),\n",
    "                            Dense(num_classes),\n",
    "                            Dropout(dropout_rate),\n",
    "                            Dense(num_classes),\n",
    "                            LogSoftmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "handled-advice",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ouput_shape= (-1, 2)\n"
     ]
    }
   ],
   "source": [
    "input_shape=(-1,)+ data_ss.shape[1:]\n",
    "output_shape, params = init_fun(key, input_shape)\n",
    "print(\"ouput_shape=\",output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "sorted-transaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(x, k, dtype=np.float32):\n",
    "    \"\"\"Create a one-hot encoding of x of size k \"\"\"\n",
    "    return np.array(x[:, None] == np.arange(k), dtype)\n",
    "\n",
    "def loss(params, data, targets,key):\n",
    "    preds = net(params, data,rng=key)\n",
    "    return -np.sum(preds * targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "subject-horizontal",
   "metadata": {},
   "outputs": [],
   "source": [
    "step_size = 1e-3\n",
    "opt_init, opt_update, get_params = optimizers.adam(step_size)\n",
    "opt_state = opt_init(params)\n",
    "num_epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "negative-split",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def update(params, x, y, opt_state,key):\n",
    "    \"\"\" Compute the gradient for a batch and update the parameters \"\"\"\n",
    "    value, grads = value_and_grad(loss)(params, x, y,key)\n",
    "    opt_state = opt_update(0, grads, opt_state)\n",
    "    return get_params(opt_state), opt_state, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "blessed-soundtrack",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 2)\n"
     ]
    }
   ],
   "source": [
    "labels_onehot=one_hot(labels_np,num_classes)\n",
    "print(labels_onehot.shape)\n",
    "def run_training_loop(num_epochs, opt_state):\n",
    "    \"\"\" Implements a learning loop over epochs. \"\"\"\n",
    "    # Get the initial set of parameters\n",
    "    params = get_params(opt_state)\n",
    "\n",
    "    # Loop over the training epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        params, opt_state, loss = update(params, data_ss, labels_onehot, opt_state,key)\n",
    "        epoch_time = time.time() - start_time\n",
    "        print(\"Epoch {} | T: {:0.2f} | loss: {:0.3f} \".format(epoch+1, epoch_time,\n",
    "                                                                    loss))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "combined-latin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | T: 0.00 | loss: 97.390 \n",
      "Epoch 2 | T: 0.00 | loss: 92.775 \n",
      "Epoch 3 | T: 0.00 | loss: 86.724 \n",
      "Epoch 4 | T: 0.00 | loss: 79.923 \n",
      "Epoch 5 | T: 0.00 | loss: 72.797 \n",
      "Epoch 6 | T: 0.00 | loss: 65.648 \n",
      "Epoch 7 | T: 0.00 | loss: 58.700 \n",
      "Epoch 8 | T: 0.00 | loss: 52.161 \n",
      "Epoch 9 | T: 0.00 | loss: 46.281 \n",
      "Epoch 10 | T: 0.00 | loss: 41.202 \n",
      "Epoch 11 | T: 0.00 | loss: 36.831 \n",
      "Epoch 12 | T: 0.00 | loss: 33.016 \n",
      "Epoch 13 | T: 0.00 | loss: 29.690 \n",
      "Epoch 14 | T: 0.00 | loss: 26.864 \n",
      "Epoch 15 | T: 0.00 | loss: 24.504 \n",
      "Epoch 16 | T: 0.00 | loss: 22.483 \n",
      "Epoch 17 | T: 0.00 | loss: 20.689 \n",
      "Epoch 18 | T: 0.00 | loss: 19.099 \n",
      "Epoch 19 | T: 0.00 | loss: 17.756 \n",
      "Epoch 20 | T: 0.00 | loss: 16.691 \n",
      "Epoch 21 | T: 0.00 | loss: 15.853 \n",
      "Epoch 22 | T: 0.00 | loss: 15.162 \n",
      "Epoch 23 | T: 0.00 | loss: 14.565 \n",
      "Epoch 24 | T: 0.00 | loss: 14.035 \n",
      "Epoch 25 | T: 0.00 | loss: 13.562 \n",
      "Epoch 26 | T: 0.00 | loss: 13.139 \n",
      "Epoch 27 | T: 0.00 | loss: 12.760 \n",
      "Epoch 28 | T: 0.00 | loss: 12.421 \n",
      "Epoch 29 | T: 0.00 | loss: 12.116 \n",
      "Epoch 30 | T: 0.00 | loss: 11.842 \n",
      "Epoch 31 | T: 0.00 | loss: 11.593 \n",
      "Epoch 32 | T: 0.00 | loss: 11.368 \n",
      "Epoch 33 | T: 0.00 | loss: 11.162 \n",
      "Epoch 34 | T: 0.00 | loss: 10.973 \n",
      "Epoch 35 | T: 0.00 | loss: 10.800 \n",
      "Epoch 36 | T: 0.00 | loss: 10.640 \n",
      "Epoch 37 | T: 0.00 | loss: 10.493 \n",
      "Epoch 38 | T: 0.00 | loss: 10.357 \n",
      "Epoch 39 | T: 0.00 | loss: 10.232 \n",
      "Epoch 40 | T: 0.00 | loss: 10.117 \n",
      "Epoch 41 | T: 0.00 | loss: 10.011 \n",
      "Epoch 42 | T: 0.00 | loss: 9.913 \n",
      "Epoch 43 | T: 0.00 | loss: 9.823 \n",
      "Epoch 44 | T: 0.00 | loss: 9.740 \n",
      "Epoch 45 | T: 0.00 | loss: 9.663 \n",
      "Epoch 46 | T: 0.00 | loss: 9.593 \n",
      "Epoch 47 | T: 0.00 | loss: 9.527 \n",
      "Epoch 48 | T: 0.00 | loss: 9.467 \n",
      "Epoch 49 | T: 0.00 | loss: 9.410 \n",
      "Epoch 50 | T: 0.00 | loss: 9.358 \n",
      "Epoch 51 | T: 0.00 | loss: 9.309 \n",
      "Epoch 52 | T: 0.00 | loss: 9.264 \n",
      "Epoch 53 | T: 0.00 | loss: 9.222 \n",
      "Epoch 54 | T: 0.00 | loss: 9.182 \n",
      "Epoch 55 | T: 0.00 | loss: 9.146 \n",
      "Epoch 56 | T: 0.00 | loss: 9.111 \n",
      "Epoch 57 | T: 0.00 | loss: 9.078 \n",
      "Epoch 58 | T: 0.00 | loss: 9.048 \n",
      "Epoch 59 | T: 0.00 | loss: 9.019 \n",
      "Epoch 60 | T: 0.00 | loss: 8.992 \n",
      "Epoch 61 | T: 0.00 | loss: 8.967 \n",
      "Epoch 62 | T: 0.00 | loss: 8.943 \n",
      "Epoch 63 | T: 0.00 | loss: 8.920 \n",
      "Epoch 64 | T: 0.00 | loss: 8.899 \n",
      "Epoch 65 | T: 0.00 | loss: 8.878 \n",
      "Epoch 66 | T: 0.00 | loss: 8.859 \n",
      "Epoch 67 | T: 0.00 | loss: 8.841 \n",
      "Epoch 68 | T: 0.00 | loss: 8.824 \n",
      "Epoch 69 | T: 0.00 | loss: 8.807 \n",
      "Epoch 70 | T: 0.00 | loss: 8.791 \n",
      "Epoch 71 | T: 0.00 | loss: 8.776 \n",
      "Epoch 72 | T: 0.00 | loss: 8.762 \n",
      "Epoch 73 | T: 0.00 | loss: 8.749 \n",
      "Epoch 74 | T: 0.00 | loss: 8.736 \n",
      "Epoch 75 | T: 0.00 | loss: 8.723 \n",
      "Epoch 76 | T: 0.00 | loss: 8.711 \n",
      "Epoch 77 | T: 0.00 | loss: 8.700 \n",
      "Epoch 78 | T: 0.00 | loss: 8.689 \n",
      "Epoch 79 | T: 0.00 | loss: 8.678 \n",
      "Epoch 80 | T: 0.00 | loss: 8.668 \n",
      "Epoch 81 | T: 0.00 | loss: 8.658 \n",
      "Epoch 82 | T: 0.00 | loss: 8.649 \n",
      "Epoch 83 | T: 0.00 | loss: 8.640 \n",
      "Epoch 84 | T: 0.00 | loss: 8.631 \n",
      "Epoch 85 | T: 0.00 | loss: 8.622 \n",
      "Epoch 86 | T: 0.00 | loss: 8.614 \n",
      "Epoch 87 | T: 0.00 | loss: 8.606 \n",
      "Epoch 88 | T: 0.00 | loss: 8.599 \n",
      "Epoch 89 | T: 0.00 | loss: 8.591 \n",
      "Epoch 90 | T: 0.00 | loss: 8.584 \n",
      "Epoch 91 | T: 0.00 | loss: 8.577 \n",
      "Epoch 92 | T: 0.00 | loss: 8.570 \n",
      "Epoch 93 | T: 0.00 | loss: 8.564 \n",
      "Epoch 94 | T: 0.00 | loss: 8.557 \n",
      "Epoch 95 | T: 0.00 | loss: 8.551 \n",
      "Epoch 96 | T: 0.00 | loss: 8.545 \n",
      "Epoch 97 | T: 0.00 | loss: 8.539 \n",
      "Epoch 98 | T: 0.00 | loss: 8.534 \n",
      "Epoch 99 | T: 0.00 | loss: 8.528 \n",
      "Epoch 100 | T: 0.00 | loss: 8.523 \n",
      "Epoch 101 | T: 0.00 | loss: 8.518 \n",
      "Epoch 102 | T: 0.00 | loss: 8.513 \n",
      "Epoch 103 | T: 0.00 | loss: 8.508 \n",
      "Epoch 104 | T: 0.00 | loss: 8.503 \n",
      "Epoch 105 | T: 0.00 | loss: 8.498 \n",
      "Epoch 106 | T: 0.00 | loss: 8.493 \n",
      "Epoch 107 | T: 0.00 | loss: 8.489 \n",
      "Epoch 108 | T: 0.00 | loss: 8.485 \n",
      "Epoch 109 | T: 0.00 | loss: 8.480 \n",
      "Epoch 110 | T: 0.00 | loss: 8.476 \n",
      "Epoch 111 | T: 0.00 | loss: 8.472 \n",
      "Epoch 112 | T: 0.00 | loss: 8.468 \n",
      "Epoch 113 | T: 0.00 | loss: 8.464 \n",
      "Epoch 114 | T: 0.00 | loss: 8.460 \n",
      "Epoch 115 | T: 0.00 | loss: 8.457 \n",
      "Epoch 116 | T: 0.00 | loss: 8.453 \n",
      "Epoch 117 | T: 0.00 | loss: 8.449 \n",
      "Epoch 118 | T: 0.00 | loss: 8.446 \n",
      "Epoch 119 | T: 0.00 | loss: 8.443 \n",
      "Epoch 120 | T: 0.00 | loss: 8.439 \n",
      "Epoch 121 | T: 0.00 | loss: 8.436 \n",
      "Epoch 122 | T: 0.00 | loss: 8.433 \n",
      "Epoch 123 | T: 0.00 | loss: 8.430 \n",
      "Epoch 124 | T: 0.00 | loss: 8.426 \n",
      "Epoch 125 | T: 0.00 | loss: 8.423 \n",
      "Epoch 126 | T: 0.00 | loss: 8.420 \n",
      "Epoch 127 | T: 0.00 | loss: 8.418 \n",
      "Epoch 128 | T: 0.00 | loss: 8.415 \n",
      "Epoch 129 | T: 0.00 | loss: 8.412 \n",
      "Epoch 130 | T: 0.00 | loss: 8.409 \n",
      "Epoch 131 | T: 0.00 | loss: 8.406 \n",
      "Epoch 132 | T: 0.00 | loss: 8.404 \n",
      "Epoch 133 | T: 0.00 | loss: 8.401 \n",
      "Epoch 134 | T: 0.00 | loss: 8.399 \n",
      "Epoch 135 | T: 0.00 | loss: 8.396 \n",
      "Epoch 136 | T: 0.00 | loss: 8.394 \n",
      "Epoch 137 | T: 0.00 | loss: 8.391 \n",
      "Epoch 138 | T: 0.00 | loss: 8.389 \n",
      "Epoch 139 | T: 0.00 | loss: 8.387 \n",
      "Epoch 140 | T: 0.00 | loss: 8.384 \n",
      "Epoch 141 | T: 0.00 | loss: 8.382 \n",
      "Epoch 142 | T: 0.00 | loss: 8.380 \n",
      "Epoch 143 | T: 0.00 | loss: 8.378 \n",
      "Epoch 144 | T: 0.00 | loss: 8.376 \n",
      "Epoch 145 | T: 0.00 | loss: 8.374 \n",
      "Epoch 146 | T: 0.00 | loss: 8.371 \n",
      "Epoch 147 | T: 0.00 | loss: 8.369 \n",
      "Epoch 148 | T: 0.00 | loss: 8.367 \n",
      "Epoch 149 | T: 0.00 | loss: 8.365 \n",
      "Epoch 150 | T: 0.00 | loss: 8.364 \n",
      "Epoch 151 | T: 0.00 | loss: 8.362 \n",
      "Epoch 152 | T: 0.00 | loss: 8.360 \n",
      "Epoch 153 | T: 0.00 | loss: 8.358 \n",
      "Epoch 154 | T: 0.00 | loss: 8.356 \n",
      "Epoch 155 | T: 0.00 | loss: 8.354 \n",
      "Epoch 156 | T: 0.00 | loss: 8.353 \n",
      "Epoch 157 | T: 0.00 | loss: 8.351 \n",
      "Epoch 158 | T: 0.00 | loss: 8.349 \n",
      "Epoch 159 | T: 0.00 | loss: 8.347 \n",
      "Epoch 160 | T: 0.00 | loss: 8.346 \n",
      "Epoch 161 | T: 0.00 | loss: 8.344 \n",
      "Epoch 162 | T: 0.00 | loss: 8.343 \n",
      "Epoch 163 | T: 0.00 | loss: 8.341 \n",
      "Epoch 164 | T: 0.00 | loss: 8.339 \n",
      "Epoch 165 | T: 0.00 | loss: 8.338 \n",
      "Epoch 166 | T: 0.00 | loss: 8.336 \n",
      "Epoch 167 | T: 0.00 | loss: 8.335 \n",
      "Epoch 168 | T: 0.00 | loss: 8.333 \n",
      "Epoch 169 | T: 0.00 | loss: 8.332 \n",
      "Epoch 170 | T: 0.00 | loss: 8.331 \n",
      "Epoch 171 | T: 0.00 | loss: 8.329 \n",
      "Epoch 172 | T: 0.00 | loss: 8.328 \n",
      "Epoch 173 | T: 0.00 | loss: 8.326 \n",
      "Epoch 174 | T: 0.00 | loss: 8.325 \n",
      "Epoch 175 | T: 0.00 | loss: 8.324 \n",
      "Epoch 176 | T: 0.00 | loss: 8.322 \n",
      "Epoch 177 | T: 0.00 | loss: 8.321 \n",
      "Epoch 178 | T: 0.00 | loss: 8.320 \n",
      "Epoch 179 | T: 0.00 | loss: 8.319 \n",
      "Epoch 180 | T: 0.00 | loss: 8.317 \n",
      "Epoch 181 | T: 0.00 | loss: 8.316 \n",
      "Epoch 182 | T: 0.00 | loss: 8.315 \n",
      "Epoch 183 | T: 0.00 | loss: 8.314 \n",
      "Epoch 184 | T: 0.00 | loss: 8.313 \n",
      "Epoch 185 | T: 0.00 | loss: 8.311 \n",
      "Epoch 186 | T: 0.00 | loss: 8.310 \n",
      "Epoch 187 | T: 0.00 | loss: 8.309 \n",
      "Epoch 188 | T: 0.00 | loss: 8.308 \n",
      "Epoch 189 | T: 0.00 | loss: 8.307 \n",
      "Epoch 190 | T: 0.00 | loss: 8.306 \n",
      "Epoch 191 | T: 0.00 | loss: 8.305 \n",
      "Epoch 192 | T: 0.00 | loss: 8.304 \n",
      "Epoch 193 | T: 0.00 | loss: 8.303 \n",
      "Epoch 194 | T: 0.00 | loss: 8.302 \n",
      "Epoch 195 | T: 0.00 | loss: 8.301 \n",
      "Epoch 196 | T: 0.00 | loss: 8.300 \n",
      "Epoch 197 | T: 0.00 | loss: 8.299 \n",
      "Epoch 198 | T: 0.00 | loss: 8.298 \n",
      "Epoch 199 | T: 0.00 | loss: 8.297 \n",
      "Epoch 200 | T: 0.00 | loss: 8.296 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeviceArray(8.295582, dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_training_loop(200,opt_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "furnished-ecuador",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
