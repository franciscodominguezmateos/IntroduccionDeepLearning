{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Francisco Dominguez Mateos\n",
    "# 08/08/2020\n",
    "# GON Variational\n",
    "# from: https://github.com/cwkx/GON/blob/master/Variational-GON.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dir = 'imgs'\n",
    "os.makedirs(plot_dir, exist_ok=True)\n",
    "\n",
    "# image data\n",
    "img_size = 32\n",
    "nc = 1\n",
    "\n",
    "# training info\n",
    "lr = 1e-4\n",
    "batch_size = 64\n",
    "nz = 48\n",
    "ngf = 32\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create GON network\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.fc21 = nn.Linear(nz, nz)\n",
    "        self.fc22 = nn.Linear(nz, nz)\n",
    "        \n",
    "        self.main = nn.Sequential(\n",
    "            nn.ConvTranspose2d(nz, ngf * 4, 4, 1, 0, bias=True),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=True),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=True),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=True),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = logvar.mul(0.5).exp_()\n",
    "            eps = std.data.new(std.size()).normal_()\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def forward(self, input):\n",
    "        mu = self.fc21(input)\n",
    "        logvar = self.fc22(input)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.main(z.unsqueeze(-1).unsqueeze(-1)), mu, logvar\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        z = torch.randn(batch_size, nz, 1, 1).cuda()\n",
    "        return self.main(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(p, x, mu, logvar, weight=0.0):\n",
    "    BCE = torch.nn.functional.binary_cross_entropy(p.view(-1, 32 * 32 * nc), x.view(-1, 32 * 32 * nc), reduction='none').sum(1).mean()\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1).mean()\n",
    "\n",
    "    return BCE + (KLD * weight), BCE, KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torchvision.datasets.FashionMNIST('data', train=True, download=True, transform=torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(img_size), torchvision.transforms.ToTensor()\n",
    "]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset, sampler=None, shuffle=True, batch_size=batch_size, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "F = Generator().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Number of parameters 268033\n"
     ]
    }
   ],
   "source": [
    "optim = torch.optim.Adam(lr=lr, params=F.parameters())\n",
    "print(f'> Number of parameters {len(torch.nn.utils.parameters_to_vector(F.parameters()))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0  Loss: 465.958\n",
      "Epoch: 1  Loss: 393.420\n",
      "Epoch: 2  Loss: 377.858\n",
      "Epoch: 3  Loss: 370.579\n",
      "Epoch: 4  Loss: 367.545\n",
      "Epoch: 5  Loss: 365.395\n",
      "Epoch: 6  Loss: 363.436\n",
      "Epoch: 7  Loss: 361.213\n",
      "Epoch: 8  Loss: 359.323\n",
      "Epoch: 9  Loss: 357.434\n",
      "Epoch: 10  Loss: 355.744\n",
      "Epoch: 11  Loss: 354.197\n",
      "Epoch: 12  Loss: 353.043\n",
      "Epoch: 13  Loss: 351.799\n",
      "Epoch: 14  Loss: 350.575\n",
      "Epoch: 15  Loss: 349.482\n",
      "Epoch: 16  Loss: 348.370\n",
      "Epoch: 17  Loss: 347.279\n",
      "Epoch: 18  Loss: 346.372\n",
      "Epoch: 19  Loss: 345.365\n",
      "Epoch: 20  Loss: 344.536\n",
      "Epoch: 21  Loss: 343.717\n",
      "Epoch: 22  Loss: 342.789\n",
      "Epoch: 23  Loss: 342.120\n",
      "Epoch: 24  Loss: 341.461\n",
      "Epoch: 25  Loss: 340.571\n",
      "Epoch: 26  Loss: 340.157\n",
      "Epoch: 27  Loss: 339.535\n",
      "Epoch: 28  Loss: 338.919\n",
      "Epoch: 29  Loss: 338.334\n",
      "Epoch: 30  Loss: 337.977\n",
      "Epoch: 31  Loss: 337.529\n",
      "Epoch: 32  Loss: 337.010\n",
      "Epoch: 33  Loss: 336.725\n",
      "Epoch: 34  Loss: 336.345\n",
      "Epoch: 35  Loss: 335.965\n",
      "Epoch: 36  Loss: 335.763\n",
      "Epoch: 37  Loss: 335.453\n",
      "Epoch: 38  Loss: 335.134\n",
      "Epoch: 39  Loss: 334.903\n",
      "Epoch: 40  Loss: 334.756\n",
      "Epoch: 41  Loss: 334.561\n",
      "Epoch: 42  Loss: 334.351\n",
      "Epoch: 43  Loss: 334.164\n",
      "Epoch: 44  Loss: 333.901\n",
      "Epoch: 45  Loss: 333.706\n",
      "Epoch: 46  Loss: 333.563\n",
      "Epoch: 47  Loss: 333.368\n",
      "Epoch: 48  Loss: 333.104\n",
      "Epoch: 49  Loss: 332.966\n",
      "Epoch: 50  Loss: 332.643\n",
      "Epoch: 51  Loss: 332.472\n",
      "Epoch: 52  Loss: 332.133\n",
      "Epoch: 53  Loss: 331.959\n",
      "Epoch: 54  Loss: 331.683\n",
      "Epoch: 55  Loss: 331.532\n",
      "Epoch: 56  Loss: 331.327\n",
      "Epoch: 57  Loss: 331.232\n",
      "Epoch: 58  Loss: 330.973\n",
      "Epoch: 59  Loss: 330.734\n",
      "Epoch: 60  Loss: 330.740\n",
      "Epoch: 61  Loss: 330.456\n",
      "Epoch: 62  Loss: 330.243\n",
      "Epoch: 63  Loss: 330.170\n",
      "Epoch: 64  Loss: 330.000\n",
      "Epoch: 65  Loss: 329.853\n",
      "Epoch: 66  Loss: 329.723\n",
      "Epoch: 67  Loss: 329.658\n",
      "Epoch: 68  Loss: 329.521\n",
      "Epoch: 69  Loss: 329.450\n",
      "Epoch: 70  Loss: 329.368\n",
      "Epoch: 71  Loss: 329.297\n",
      "Epoch: 72  Loss: 329.201\n",
      "Epoch: 73  Loss: 329.064\n",
      "Epoch: 74  Loss: 328.923\n",
      "Epoch: 75  Loss: 328.926\n",
      "Epoch: 76  Loss: 328.801\n",
      "Epoch: 77  Loss: 328.639\n",
      "Epoch: 78  Loss: 328.527\n",
      "Epoch: 79  Loss: 328.471\n",
      "Epoch: 80  Loss: 328.391\n",
      "Epoch: 81  Loss: 328.290\n",
      "Epoch: 82  Loss: 328.170\n",
      "Epoch: 83  Loss: 328.010\n",
      "Epoch: 84  Loss: 327.969\n",
      "Epoch: 85  Loss: 327.952\n",
      "Epoch: 86  Loss: 327.839\n",
      "Epoch: 87  Loss: 327.744\n",
      "Epoch: 88  Loss: 327.775\n",
      "Epoch: 89  Loss: 327.644\n",
      "Epoch: 90  Loss: 327.602\n",
      "Epoch: 91  Loss: 327.495\n",
      "Epoch: 92  Loss: 327.443\n",
      "Epoch: 93  Loss: 327.410\n",
      "Epoch: 94  Loss: 327.353\n",
      "Epoch: 95  Loss: 327.284\n",
      "Epoch: 96  Loss: 327.201\n",
      "Epoch: 97  Loss: 327.120\n",
      "Epoch: 98  Loss: 327.114\n",
      "Epoch: 99  Loss: 327.050\n",
      "Epoch: 100  Loss: 327.069\n",
      "Epoch: 101  Loss: 327.024\n",
      "Epoch: 102  Loss: 326.943\n",
      "Epoch: 103  Loss: 326.911\n",
      "Epoch: 104  Loss: 326.856\n",
      "Epoch: 105  Loss: 326.764\n",
      "Epoch: 106  Loss: 326.737\n",
      "Epoch: 107  Loss: 326.663\n",
      "Epoch: 108  Loss: 326.688\n",
      "Epoch: 109  Loss: 326.649\n",
      "Epoch: 110  Loss: 326.560\n",
      "Epoch: 111  Loss: 326.526\n",
      "Epoch: 112  Loss: 326.459\n",
      "Epoch: 113  Loss: 326.448\n",
      "Epoch: 114  Loss: 326.427\n",
      "Epoch: 115  Loss: 326.433\n",
      "Epoch: 116  Loss: 326.340\n",
      "Epoch: 117  Loss: 326.348\n",
      "Epoch: 118  Loss: 326.259\n",
      "Epoch: 119  Loss: 326.237\n",
      "Epoch: 120  Loss: 326.221\n",
      "Epoch: 121  Loss: 326.208\n",
      "Epoch: 122  Loss: 326.141\n",
      "Epoch: 123  Loss: 326.111\n",
      "Epoch: 124  Loss: 326.070\n",
      "Epoch: 125  Loss: 326.088\n",
      "Epoch: 126  Loss: 326.033\n",
      "Epoch: 127  Loss: 325.994\n",
      "Epoch: 128  Loss: 325.937\n",
      "Epoch: 129  Loss: 325.902\n",
      "Epoch: 130  Loss: 325.916\n",
      "Epoch: 131  Loss: 325.883\n",
      "Epoch: 132  Loss: 325.809\n",
      "Epoch: 133  Loss: 325.800\n",
      "Epoch: 134  Loss: 325.808\n",
      "Epoch: 135  Loss: 325.760\n",
      "Epoch: 136  Loss: 325.740\n",
      "Epoch: 137  Loss: 325.714\n",
      "Epoch: 138  Loss: 325.695\n",
      "Epoch: 139  Loss: 325.677\n",
      "Epoch: 140  Loss: 325.699\n",
      "Epoch: 141  Loss: 325.595\n",
      "Epoch: 142  Loss: 325.544\n",
      "Epoch: 143  Loss: 325.480\n",
      "Epoch: 144  Loss: 325.507\n",
      "Epoch: 145  Loss: 325.523\n",
      "Epoch: 146  Loss: 325.503\n",
      "Epoch: 147  Loss: 325.475\n",
      "Epoch: 148  Loss: 325.421\n",
      "Epoch: 149  Loss: 325.414\n",
      "Epoch: 150  Loss: 325.380\n",
      "Epoch: 151  Loss: 325.386\n",
      "Epoch: 152  Loss: 325.399\n",
      "Epoch: 153  Loss: 325.306\n",
      "Epoch: 154  Loss: 325.261\n",
      "Epoch: 155  Loss: 325.292\n",
      "Epoch: 156  Loss: 325.285\n",
      "Epoch: 157  Loss: 325.249\n",
      "Epoch: 158  Loss: 325.257\n",
      "Epoch: 159  Loss: 325.200\n",
      "Epoch: 160  Loss: 325.165\n",
      "Epoch: 161  Loss: 325.128\n",
      "Epoch: 162  Loss: 325.194\n",
      "Epoch: 163  Loss: 325.102\n",
      "Epoch: 164  Loss: 325.099\n",
      "Epoch: 165  Loss: 325.092\n",
      "Epoch: 166  Loss: 325.089\n",
      "Epoch: 167  Loss: 325.098\n",
      "Epoch: 168  Loss: 325.106\n",
      "Epoch: 169  Loss: 325.030\n",
      "Epoch: 170  Loss: 325.037\n",
      "Epoch: 171  Loss: 324.985\n",
      "Epoch: 172  Loss: 325.006\n",
      "Epoch: 173  Loss: 324.964\n",
      "Epoch: 174  Loss: 325.003\n",
      "Epoch: 175  Loss: 324.969\n",
      "Epoch: 176  Loss: 324.949\n",
      "Epoch: 177  Loss: 324.935\n",
      "Epoch: 178  Loss: 324.893\n",
      "Epoch: 179  Loss: 324.895\n",
      "Epoch: 180  Loss: 324.865\n",
      "Epoch: 181  Loss: 324.818\n",
      "Epoch: 182  Loss: 324.808\n",
      "Epoch: 183  Loss: 324.784\n",
      "Epoch: 184  Loss: 324.771\n",
      "Epoch: 185  Loss: 324.768\n",
      "Epoch: 186  Loss: 324.716\n",
      "Epoch: 187  Loss: 324.722\n",
      "Epoch: 188  Loss: 324.676\n",
      "Epoch: 189  Loss: 324.702\n",
      "Epoch: 190  Loss: 324.696\n",
      "Epoch: 191  Loss: 324.683\n",
      "Epoch: 192  Loss: 324.645\n",
      "Epoch: 193  Loss: 324.657\n",
      "Epoch: 194  Loss: 324.650\n",
      "Epoch: 195  Loss: 324.650\n",
      "Epoch: 196  Loss: 324.621\n",
      "Epoch: 197  Loss: 324.618\n",
      "Epoch: 198  Loss: 324.594\n",
      "Epoch: 199  Loss: 324.571\n",
      "Epoch: 200  Loss: 324.587\n",
      "Epoch: 201  Loss: 324.538\n",
      "Epoch: 202  Loss: 324.496\n",
      "Epoch: 203  Loss: 324.533\n",
      "Epoch: 204  Loss: 324.508\n",
      "Epoch: 205  Loss: 324.467\n",
      "Epoch: 206  Loss: 324.507\n",
      "Epoch: 207  Loss: 324.472\n",
      "Epoch: 208  Loss: 324.453\n",
      "Epoch: 209  Loss: 324.392\n",
      "Epoch: 210  Loss: 324.385\n",
      "Epoch: 211  Loss: 324.440\n",
      "Epoch: 212  Loss: 324.398\n",
      "Epoch: 213  Loss: 324.415\n",
      "Epoch: 214  Loss: 324.395\n",
      "Epoch: 215  Loss: 324.362\n",
      "Epoch: 216  Loss: 324.331\n",
      "Epoch: 217  Loss: 324.309\n",
      "Epoch: 218  Loss: 324.257\n",
      "Epoch: 219  Loss: 324.284\n",
      "Epoch: 220  Loss: 324.275\n",
      "Epoch: 221  Loss: 324.344\n",
      "Epoch: 222  Loss: 324.276\n",
      "Epoch: 223  Loss: 324.280\n",
      "Epoch: 224  Loss: 324.294\n",
      "Epoch: 225  Loss: 324.318\n",
      "Epoch: 226  Loss: 324.297\n",
      "Epoch: 227  Loss: 324.313\n",
      "Epoch: 228  Loss: 324.319\n",
      "Epoch: 229  Loss: 324.323\n",
      "Epoch: 230  Loss: 324.252\n",
      "Epoch: 231  Loss: 324.281\n",
      "Epoch: 232  Loss: 324.264\n",
      "Epoch: 233  Loss: 324.269\n",
      "Epoch: 234  Loss: 324.256\n",
      "Epoch: 235  Loss: 324.256\n",
      "Epoch: 236  Loss: 324.223\n",
      "Epoch: 237  Loss: 324.238\n",
      "Epoch: 238  Loss: 324.182\n",
      "Epoch: 239  Loss: 324.196\n",
      "Epoch: 240  Loss: 324.192\n",
      "Epoch: 241  Loss: 324.194\n",
      "Epoch: 242  Loss: 324.164\n",
      "Epoch: 243  Loss: 324.202\n",
      "Epoch: 244  Loss: 324.154\n",
      "Epoch: 245  Loss: 324.189\n",
      "Epoch: 246  Loss: 324.134\n",
      "Epoch: 247  Loss: 324.127\n",
      "Epoch: 248  Loss: 324.121\n",
      "Epoch: 249  Loss: 324.044\n",
      "Epoch: 250  Loss: 324.089\n",
      "Epoch: 251  Loss: 324.105\n",
      "Epoch: 252  Loss: 324.081\n",
      "Epoch: 253  Loss: 324.115\n",
      "Epoch: 254  Loss: 324.066\n",
      "Epoch: 255  Loss: 324.101\n",
      "Epoch: 256  Loss: 324.012\n",
      "Epoch: 257  Loss: 324.008\n",
      "Epoch: 258  Loss: 324.027\n",
      "Epoch: 259  Loss: 324.003\n",
      "Epoch: 260  Loss: 324.025\n",
      "Epoch: 261  Loss: 324.019\n",
      "Epoch: 262  Loss: 323.991\n",
      "Epoch: 263  Loss: 323.959\n",
      "Epoch: 264  Loss: 323.965\n",
      "Epoch: 265  Loss: 323.972\n",
      "Epoch: 266  Loss: 323.990\n",
      "Epoch: 267  Loss: 323.935\n",
      "Epoch: 268  Loss: 323.965\n",
      "Epoch: 269  Loss: 323.969\n",
      "Epoch: 270  Loss: 323.934\n",
      "Epoch: 271  Loss: 323.961\n",
      "Epoch: 272  Loss: 323.923\n",
      "Epoch: 273  Loss: 323.909\n",
      "Epoch: 274  Loss: 323.985\n",
      "Epoch: 275  Loss: 323.951\n",
      "Epoch: 276  Loss: 323.890\n",
      "Epoch: 277  Loss: 323.880\n",
      "Epoch: 278  Loss: 323.873\n",
      "Epoch: 279  Loss: 323.878\n",
      "Epoch: 280  Loss: 323.861\n",
      "Epoch: 281  Loss: 323.890\n",
      "Epoch: 282  Loss: 323.878\n",
      "Epoch: 283  Loss: 323.874\n",
      "Epoch: 284  Loss: 323.853\n",
      "Epoch: 285  Loss: 323.839\n",
      "Epoch: 286  Loss: 323.867\n",
      "Epoch: 287  Loss: 323.842\n",
      "Epoch: 288  Loss: 323.758\n",
      "Epoch: 289  Loss: 323.833\n",
      "Epoch: 290  Loss: 323.786\n",
      "Epoch: 291  Loss: 323.792\n",
      "Epoch: 292  Loss: 323.791\n",
      "Epoch: 293  Loss: 323.769\n",
      "Epoch: 294  Loss: 323.762\n",
      "Epoch: 295  Loss: 323.802\n",
      "Epoch: 296  Loss: 323.737\n",
      "Epoch: 297  Loss: 323.792\n",
      "Epoch: 298  Loss: 323.761\n",
      "Epoch: 299  Loss: 323.724\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(300):\n",
    "    epoch_loss = 0.0\n",
    "    # anneal KLD from 0 to 1 over 100 epochs\n",
    "    #kld_weight = min(epoch / 99.0, 1.0)\n",
    "    for x, _ in train_loader:\n",
    "        x = x.to(device)\n",
    "\n",
    "        # compute the gradients of the inner loss with respect to zeros (gradient origin)\n",
    "        z = torch.zeros(batch_size, nz).to(device).requires_grad_()\n",
    "        g, mu, logvar = F(z)\n",
    "        inner_loss, BCE, KLD = vae_loss(g, x, mu, logvar, 1.0)\n",
    "        grad = torch.autograd.grad(inner_loss, [z], create_graph=True, retain_graph=True)[0]\n",
    "        z = (-grad)\n",
    "\n",
    "        # now with z as our new latent points, optimise the data fitting loss\n",
    "        g, mu, logvar = F(z)\n",
    "        outer_loss, BCE, KLD = vae_loss(g, x, mu, logvar, 1.0)\n",
    "        optim.zero_grad()\n",
    "        outer_loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        epoch_loss += outer_loss.item()\n",
    "    \n",
    "    print(f\"Epoch: {epoch}  Loss: {epoch_loss/len(train_loader):.3f}\")\n",
    "\n",
    "    # plot reconstructions\n",
    "    torchvision.utils.save_image(g, f'imgs/recon_{epoch}.png', \n",
    "        nrow=int(np.sqrt(batch_size)), padding=0)\n",
    "    \n",
    "    # plot samples\n",
    "    torchvision.utils.save_image(F.sample(batch_size), f'imgs/sample_{epoch}.png', \n",
    "        nrow=int(np.sqrt(batch_size)), padding=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
