{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Automatic speech recognition and speaker identity\n",
    "\"\"\"\n",
    "from __future__ import division, print_function, absolute_import\n",
    "import os\n",
    "import numpy as np\n",
    "import sklearn.preprocessing as prep\n",
    "import librosa\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Input,Dense, Dropout, LSTM, Embedding, Bidirectional\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras import utils,losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports doned\n",
      "fNames= 108\n",
      "file read= /home/francisco/voz/paco_no_006.wav\n",
      "(3871,) 16000\n",
      "file read= /home/francisco/voz/paco_si_008.wav\n",
      "(4963,) 16000\n",
      "file read= /home/francisco/voz/paco_dos_002.wav\n",
      "(9285,) 16000\n",
      "file read= /home/francisco/voz/paco_uno_004.wav\n",
      "(4434,) 16000\n",
      "file read= /home/francisco/voz/paco_no_007.wav\n",
      "(2428,) 16000\n",
      "file read= /home/francisco/voz/paco_para_003.wav\n",
      "(4882,) 16000\n",
      "file read= /home/francisco/voz/paco_cuatro_003.wav\n",
      "(6972,) 16000\n",
      "file read= /home/francisco/voz/paco_nueve_001.wav\n",
      "(6524,) 16000\n",
      "file read= /home/francisco/voz/carlos_si_002.wav\n",
      "(4630,) 16000\n",
      "file read= /home/francisco/voz/paco_buenos-dias_001.wav\n",
      "(14450,) 16000\n",
      "file read= /home/francisco/voz/paco_buenos-dias_003.wav\n",
      "(14816,) 16000\n",
      "file read= /home/francisco/voz/irene_hola_003.wav\n",
      "(6847,) 16000\n",
      "file read= /home/francisco/voz/paco_seis_001.wav\n",
      "(9176,) 16000\n",
      "file read= /home/francisco/voz/paco_buenos-dias_007.wav\n",
      "(8618,) 16000\n",
      "file read= /home/francisco/voz/paco_ocho_001.wav\n",
      "(6462,) 16000\n",
      "file read= /home/francisco/voz/paco_cinco_001.wav\n",
      "(9845,) 16000\n",
      "file read= /home/francisco/voz/carlos_hola_001.wav\n",
      "(7270,) 16000\n",
      "file read= /home/francisco/voz/paco_hola_001.wav\n",
      "(5075,) 16000\n",
      "file read= /home/francisco/voz/paco_nueve_002.wav\n",
      "(5616,) 16000\n",
      "file read= /home/francisco/voz/paco_no_005.wav\n",
      "(3896,) 16000\n",
      "file read= /home/francisco/voz/irene_hola_002.wav\n",
      "(7073,) 16000\n",
      "file read= /home/francisco/voz/paco_si_004.wav\n",
      "(5872,) 16000\n",
      "file read= /home/francisco/voz/paco_hola_002.wav\n",
      "(5357,) 16000\n",
      "file read= /home/francisco/voz/paco_avanza_001.wav\n",
      "(7849,) 16000\n",
      "file read= /home/francisco/voz/paco_siete_006.wav\n",
      "(5757,) 16000\n",
      "file read= /home/francisco/voz/paco_cuatro_004.wav\n",
      "(6423,) 16000\n",
      "file read= /home/francisco/voz/eva_si_001.wav\n",
      "(13648,) 16000\n",
      "file read= /home/francisco/voz/paco_para_002.wav\n",
      "(5589,) 16000\n",
      "file read= /home/francisco/voz/eva_si_003.wav\n",
      "(8627,) 16000\n",
      "file read= /home/francisco/voz/paco_si_002.wav\n",
      "(8331,) 16000\n",
      "file read= /home/francisco/voz/paco_si_006.wav\n",
      "(5890,) 16000\n",
      "file read= /home/francisco/voz/paco_seis_002.wav\n",
      "(9059,) 16000\n",
      "file read= /home/francisco/voz/paco_buenos-dias_006.wav\n",
      "(12950,) 16000\n",
      "file read= /home/francisco/voz/paco_cinco_005.wav\n",
      "(7160,) 16000\n",
      "file read= /home/francisco/voz/paco_ocho_003.wav\n",
      "(8972,) 16000\n",
      "file read= /home/francisco/voz/carlos_no_004.wav\n",
      "(5043,) 16000\n",
      "file read= /home/francisco/voz/paco_dos_004.wav\n",
      "(5458,) 16000\n",
      "file read= /home/francisco/voz/paco_buenos-dias_002.wav\n",
      "(12632,) 16000\n",
      "file read= /home/francisco/voz/paco_cinco_003.wav\n",
      "(9099,) 16000\n",
      "file read= /home/francisco/voz/paco_uno_003.wav\n",
      "(6113,) 16000\n",
      "file read= /home/francisco/voz/paco_nueve_005.wav\n",
      "(7262,) 16000\n",
      "file read= /home/francisco/voz/paco_si_007.wav\n",
      "(6069,) 16000\n",
      "file read= /home/francisco/voz/paco_no_004.wav\n",
      "(3702,) 16000\n",
      "file read= /home/francisco/voz/paco_casa_002.wav\n",
      "(6401,) 16000\n",
      "file read= /home/francisco/voz/carlos_hola_002.wav\n",
      "(5847,) 16000\n",
      "file read= /home/francisco/voz/carlos_no_003.wav\n",
      "(14357,) 16000\n",
      "file read= /home/francisco/voz/paco_retrocede_003.wav\n",
      "(10812,) 16000\n",
      "file read= /home/francisco/voz/carlos_si_003.wav\n",
      "(5622,) 16000\n",
      "file read= /home/francisco/voz/carlos_si_001.wav\n",
      "(7438,) 16000\n",
      "file read= /home/francisco/voz/eva_hola_001.wav\n",
      "(11913,) 16000\n",
      "file read= /home/francisco/voz/paco_tres_004.wav\n",
      "(8079,) 16000\n",
      "file read= /home/francisco/voz/paco_seis_004.wav\n",
      "(9890,) 16000\n",
      "file read= /home/francisco/voz/paco_ocho_004.wav\n",
      "(7270,) 16000\n",
      "file read= /home/francisco/voz/paco_avanza_002.wav\n",
      "(8388,) 16000\n",
      "file read= /home/francisco/voz/paco_cinco_004.wav\n",
      "(6892,) 16000\n",
      "file read= /home/francisco/voz/eva_no_002.wav\n",
      "(6376,) 16000\n",
      "file read= /home/francisco/voz/irene_hola_001.wav\n",
      "(8327,) 16000\n",
      "file read= /home/francisco/voz/paco_tres_001.wav\n",
      "(8631,) 16000\n",
      "file read= /home/francisco/voz/paco_retrocede_001.wav\n",
      "(10345,) 16000\n",
      "file read= /home/francisco/voz/paco_uno_002.wav\n",
      "(7202,) 16000\n",
      "file read= /home/francisco/voz/paco_si_001.wav\n",
      "(8066,) 16000\n",
      "file read= /home/francisco/voz/paco_avanza_003.wav\n",
      "(8532,) 16000\n",
      "file read= /home/francisco/voz/paco_uno_005.wav\n",
      "(4172,) 16000\n",
      "file read= /home/francisco/voz/paco_tres_003.wav\n",
      "(6704,) 16000\n",
      "file read= /home/francisco/voz/paco_dos_003.wav\n",
      "(6756,) 16000\n",
      "file read= /home/francisco/voz/paco_si_003.wav\n",
      "(7346,) 16000\n",
      "file read= /home/francisco/voz/paco_cuatro_001.wav\n",
      "(7641,) 16000\n",
      "file read= /home/francisco/voz/paco_cuatro_002.wav\n",
      "(7219,) 16000\n",
      "file read= /home/francisco/voz/paco_hola_003.wav\n",
      "(4615,) 16000\n",
      "file read= /home/francisco/voz/carlos_no_002.wav\n",
      "(6946,) 16000\n",
      "file read= /home/francisco/voz/paco_casa_001.wav\n",
      "(6780,) 16000\n",
      "file read= /home/francisco/voz/paco_cinco_002.wav\n",
      "(10073,) 16000\n",
      "file read= /home/francisco/voz/carlos_no_001.wav\n",
      "(16053,) 16000\n",
      "file read= /home/francisco/voz/paco_retrocede_002.wav\n",
      "(10295,) 16000\n",
      "file read= /home/francisco/voz/paco_siete_003.wav\n",
      "(9721,) 16000\n",
      "file read= /home/francisco/voz/paco_seis_005.wav\n",
      "(10343,) 16000\n",
      "file read= /home/francisco/voz/paco_cinco_006.wav\n",
      "(6120,) 16000\n",
      "file read= /home/francisco/voz/eva_hola_003.wav\n",
      "(5663,) 16000\n",
      "file read= /home/francisco/voz/paco_buenos-dias_009.wav\n",
      "(8196,) 16000\n",
      "file read= /home/francisco/voz/paco_para_001.wav\n",
      "(5703,) 16000\n",
      "file read= /home/francisco/voz/paco_buenos-dias_008.wav\n",
      "(11447,) 16000\n",
      "file read= /home/francisco/voz/paco_buenos-dias_011.wav\n",
      "(10506,) 16000\n",
      "file read= /home/francisco/voz/paco_si_005.wav\n",
      "(5693,) 16000\n",
      "file read= /home/francisco/voz/paco_dos_001.wav\n",
      "(9067,) 16000\n",
      "file read= /home/francisco/voz/paco_uno_001.wav\n",
      "(7430,) 16000\n",
      "file read= /home/francisco/voz/paco_siete_001.wav\n",
      "(10625,) 16000\n",
      "file read= /home/francisco/voz/paco_seis_003.wav\n",
      "(9775,) 16000\n",
      "file read= /home/francisco/voz/eva_si_002.wav\n",
      "(6215,) 16000\n",
      "file read= /home/francisco/voz/paco_casa_006.wav\n",
      "(7428,) 16000\n",
      "file read= /home/francisco/voz/paco_no_003.wav\n",
      "(4922,) 16000\n",
      "file read= /home/francisco/voz/paco_siete_005.wav\n",
      "(9324,) 16000\n",
      "file read= /home/francisco/voz/paco_buenos-dias_004.wav\n",
      "(10615,) 16000\n",
      "file read= /home/francisco/voz/carlos_hola_003.wav\n",
      "(5213,) 16000\n",
      "file read= /home/francisco/voz/paco_siete_002.wav\n",
      "(10391,) 16000\n",
      "file read= /home/francisco/voz/eva_hola_002.wav\n",
      "(8744,) 16000\n",
      "file read= /home/francisco/voz/paco_tres_002.wav\n",
      "(7899,) 16000\n",
      "file read= /home/francisco/voz/paco_buenos-dias_010.wav\n",
      "(11328,) 16000\n",
      "file read= /home/francisco/voz/paco_casa_004.wav\n",
      "(8430,) 16000\n",
      "file read= /home/francisco/voz/paco_ocho_002.wav\n",
      "(7916,) 16000\n",
      "file read= /home/francisco/voz/paco_no_002.wav\n",
      "(4710,) 16000\n",
      "file read= /home/francisco/voz/eva_no_001.wav\n",
      "(12107,) 16000\n",
      "file read= /home/francisco/voz/paco_nueve_004.wav\n",
      "(7368,) 16000\n",
      "file read= /home/francisco/voz/paco_casa_003.wav\n",
      "(7110,) 16000\n",
      "file read= /home/francisco/voz/paco_no_001.wav\n",
      "(4450,) 16000\n",
      "file read= /home/francisco/voz/paco_buenos-dias_005.wav\n",
      "(13038,) 16000\n",
      "file read= /home/francisco/voz/paco_siete_004.wav\n",
      "(9621,) 16000\n",
      "file read= /home/francisco/voz/paco_nueve_003.wav\n",
      "(4254,) 16000\n",
      "file read= /home/francisco/voz/paco_casa_005.wav\n",
      "(5938,) 16000\n",
      "108\n",
      "['no', 'si', 'dos', 'uno', 'no', 'para', 'cuatro', 'nueve', 'si', 'buenos-dias', 'buenos-dias', 'hola', 'seis', 'buenos-dias', 'ocho', 'cinco', 'hola', 'hola', 'nueve', 'no', 'hola', 'si', 'hola', 'avanza', 'siete', 'cuatro', 'si', 'para', 'si', 'si', 'si', 'seis', 'buenos-dias', 'cinco', 'ocho', 'no', 'dos', 'buenos-dias', 'cinco', 'uno', 'nueve', 'si', 'no', 'casa', 'hola', 'no', 'retrocede', 'si', 'si', 'hola', 'tres', 'seis', 'ocho', 'avanza', 'cinco', 'no', 'hola', 'tres', 'retrocede', 'uno', 'si', 'avanza', 'uno', 'tres', 'dos', 'si', 'cuatro', 'cuatro', 'hola', 'no', 'casa', 'cinco', 'no', 'retrocede', 'siete', 'seis', 'cinco', 'hola', 'buenos-dias', 'para', 'buenos-dias', 'buenos-dias', 'si', 'dos', 'uno', 'siete', 'seis', 'si', 'casa', 'no', 'siete', 'buenos-dias', 'hola', 'siete', 'hola', 'tres', 'buenos-dias', 'casa', 'ocho', 'no', 'no', 'nueve', 'casa', 'no', 'buenos-dias', 'siete', 'nueve', 'casa']\n",
      "['paco', 'paco', 'paco', 'paco', 'paco', 'paco', 'paco', 'paco', 'carlos', 'paco', 'paco', 'irene', 'paco', 'paco', 'paco', 'paco', 'carlos', 'paco', 'paco', 'paco', 'irene', 'paco', 'paco', 'paco', 'paco', 'paco', 'eva', 'paco', 'eva', 'paco', 'paco', 'paco', 'paco', 'paco', 'paco', 'carlos', 'paco', 'paco', 'paco', 'paco', 'paco', 'paco', 'paco', 'paco', 'carlos', 'carlos', 'paco', 'carlos', 'carlos', 'eva', 'paco', 'paco', 'paco', 'paco', 'paco', 'eva', 'irene', 'paco', 'paco', 'paco', 'paco', 'paco', 'paco', 'paco', 'paco', 'paco', 'paco', 'paco', 'paco', 'carlos', 'paco', 'paco', 'carlos', 'paco', 'paco', 'paco', 'paco', 'eva', 'paco', 'paco', 'paco', 'paco', 'paco', 'paco', 'paco', 'paco', 'paco', 'eva', 'paco', 'paco', 'paco', 'paco', 'carlos', 'paco', 'eva', 'paco', 'paco', 'paco', 'paco', 'paco', 'eva', 'paco', 'paco', 'paco', 'paco', 'paco', 'paco', 'paco']\n",
      "LabelEncoder()\n"
     ]
    }
   ],
   "source": [
    "print(\"Imports doned\")\n",
    "def wav2mfcc(file_path, max_pad_len=10):\n",
    "    wave, sr = librosa.load(file_path, mono=True, sr=None)\n",
    "    print(\"file read=\",file_path)\n",
    "    print(wave.shape,sr)\n",
    "    wave=prep.maxabs_scale(wave) #Normalize amplitude to +-1\n",
    "    mfcc = librosa.feature.mfcc(wave, sr=sr).transpose()\n",
    "    #print(mfcc.shape)\n",
    "    #print(mfcc[1,:5])\n",
    "    dif=max_pad_len-mfcc.shape[0]\n",
    "    #padding sample\n",
    "    if dif>0:\n",
    "        #fill holes\n",
    "        #print(\"filling holes\")\n",
    "        for i in range(dif):\n",
    "            mfcc=np.vstack([mfcc,np.zeros(mfcc.shape[1])])\n",
    "    else:\n",
    "        #take away excess\n",
    "        #print(\"excess\")\n",
    "        mfcc=mfcc[:max_pad_len]\n",
    "    #print(mfcc.shape)\n",
    "    #print(mfcc[199,:5])\n",
    "    return mfcc\n",
    "\n",
    "# load wav files with this format speaker_word_number.wav\n",
    "class EasyASR(object):\n",
    "    '''\n",
    "    classdocs\n",
    "    '''\n",
    "    def __init__(self, filePath=\"/home/francisco/voz\"):\n",
    "        '''\n",
    "        Constructor\n",
    "        '''\n",
    "        basepath=filePath\n",
    "        self.mfccs=[]\n",
    "        self.speakers=[]\n",
    "        self.words=[]\n",
    "        fNames=os.listdir(basepath)\n",
    "        print(\"fNames=\",len(fNames))\n",
    "        for i,fn in enumerate(fNames):\n",
    "            #file name format speaker_word_number.wav\n",
    "            n,_=fn.split(\".\")#take away extension .wav\n",
    "            mfccs=wav2mfcc(basepath+'/{}.wav'.format(n))\n",
    "            self.mfccs.append(mfccs)\n",
    "            #decode file name format speaker_word_number\n",
    "            speaker,word,number=n.split(\"_\")\n",
    "            self.speakers.append(speaker)\n",
    "            self.words.append(word)\n",
    "        self.leSpeakers=prep.LabelEncoder()\n",
    "        self.leSpeakers.fit(self.speakers)\n",
    "        self.iSpeakerLabels=self.leSpeakers.transform(self.speakers)\n",
    "        self.leWords=prep.LabelEncoder()\n",
    "        self.leWords.fit(self.words)\n",
    "        self.iWordLabels=self.leWords.transform(self.words)\n",
    "        print(len(self.words))\n",
    "        print(self.words)\n",
    "        print(self.speakers)\n",
    "\n",
    "ea=EasyASR()\n",
    "print(ea.leWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17 4\n",
      "(108, 10, 20) (108, 17) (108, 4)\n"
     ]
    }
   ],
   "source": [
    "trainX, trainYwords, trainYspeakers = np.array(ea.mfccs),np.array(ea.iWordLabels),np.array(ea.iSpeakerLabels)\n",
    "#testX, testY = test\n",
    "\n",
    "maxlen=trainX.shape[1]\n",
    "lstm_hidden_units=128\n",
    "input_dim=trainX.shape[2]\n",
    "outputWords_dim=max(trainYwords)+1\n",
    "outputSpeakers_dim=max(trainYspeakers)+1\n",
    "\n",
    "print(outputWords_dim,outputSpeakers_dim)\n",
    "\n",
    "# Converting labels to binary vectors\n",
    "trainYwords_cat    = utils.to_categorical(trainYwords   ,outputWords_dim)\n",
    "trainYspeakers_cat = utils.to_categorical(trainYspeakers,outputSpeakers_dim)\n",
    "print (trainX.shape,trainYwords_cat.shape,trainYspeakers_cat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_10\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_19 (InputLayer)           [(None, None, 20)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, None, 128)    7808        input_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)              (None, None, 128)    49280       conv1d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_10 (Global (None, 128)          0           conv1d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_33 (Dense)                (None, 128)          16512       global_max_pooling1d_10[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_36 (Dropout)            (None, 128)          0           dense_33[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "prediction_words (Dense)        (None, 17)           2193        dropout_36[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "prediction_speakers (Dense)     (None, 4)            516         dropout_36[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 76,309\n",
      "Trainable params: 76,309\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Network building\n",
    "def build_model_asr_conv1d(input_dim,outputWords_dim,outputSpeakers_dim,hiden_dim=128):\n",
    "    inputs = tf.keras.Input(shape=(None,input_dim))\n",
    "\n",
    "    # Conv1D + global max pooling\n",
    "    x = layers.Conv1D(hiden_dim, 3, padding=\"valid\", activation=\"relu\", strides=1)(inputs)\n",
    "    #x = layers.Dropout(0.5)(x)\n",
    "    x = layers.Conv1D(hiden_dim, 3, padding=\"valid\", activation=\"relu\", strides=1)(x)\n",
    "    #x = layers.Dropout(0.5)(x)\n",
    "    #x = layers.Conv1D(hiden_dim, 3, padding=\"valid\", activation=\"relu\", strides=1)(x)\n",
    "    x = layers.GlobalMaxPooling1D()(x)\n",
    "\n",
    "    # We add a vanilla hidden layer:\n",
    "    x = layers.Dense(hiden_dim, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "\n",
    "    # from: https://www.tensorflow.org/guide/keras/train_and_evaluate\n",
    "    #       Passing data to multi-input, multi-output models\n",
    "    prediction_words    = layers.Dense(outputWords_dim   , activation=\"softmax\", name=\"prediction_words\")(x)\n",
    "    prediction_speakers = layers.Dense(outputSpeakers_dim, activation=\"softmax\", name=\"prediction_speakers\")(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs, [prediction_words,prediction_speakers])\n",
    "    return model\n",
    "\n",
    "def build_model_asr_lstm(input_dim,outputs,hiden_dim=128):\n",
    "    input = Input(shape=(None,maxlen,input_dim))\n",
    "    x,h,c = LSTM(hiden_dim, input_dim=input_dim, input_length=maxlen)(input)\n",
    "    #x = Bidirectional()(x)\n",
    "    x = Dropout(0.65)(x)\n",
    "    x = Dense(outputs,activation=\"softmax\")(x)\n",
    "    return Model(input, x)\n",
    "\n",
    "model=build_model_asr_conv1d(input_dim,outputWords_dim,outputSpeakers_dim)\n",
    "#model=build_model_asr_conv1d(input_dim,outputWords_dim,outputSpeakers_dim)\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "Train on 108 samples\n",
      "Epoch 1/50\n",
      "108/108 [==============================] - 1s 6ms/sample - loss: 16.9806 - prediction_words_loss: 13.1862 - prediction_speakers_loss: 3.7944 - prediction_words_accuracy: 0.0926 - prediction_speakers_accuracy: 0.5093  \n",
      "Epoch 2/50\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 3.8094 - prediction_words_loss: 2.9104 - prediction_speakers_loss: 0.8989 - prediction_words_accuracy: 0.2685 - prediction_speakers_accuracy: 0.7685\n",
      "Epoch 3/50\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 2.9663 - prediction_words_loss: 2.1161 - prediction_speakers_loss: 0.8502 - prediction_words_accuracy: 0.3611 - prediction_speakers_accuracy: 0.7593\n",
      "Epoch 4/50\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 2.9908 - prediction_words_loss: 2.3111 - prediction_speakers_loss: 0.6796 - prediction_words_accuracy: 0.3611 - prediction_speakers_accuracy: 0.7407\n",
      "Epoch 5/50\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 2.4463 - prediction_words_loss: 1.8793 - prediction_speakers_loss: 0.5670 - prediction_words_accuracy: 0.4444 - prediction_speakers_accuracy: 0.8148\n",
      "Epoch 6/50\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 2.1051 - prediction_words_loss: 1.7409 - prediction_speakers_loss: 0.3642 - prediction_words_accuracy: 0.4815 - prediction_speakers_accuracy: 0.8796\n",
      "Epoch 7/50\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 1.5017 - prediction_words_loss: 1.1959 - prediction_speakers_loss: 0.3058 - prediction_words_accuracy: 0.6111 - prediction_speakers_accuracy: 0.8981\n",
      "Epoch 8/50\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 1.5316 - prediction_words_loss: 1.2655 - prediction_speakers_loss: 0.2661 - prediction_words_accuracy: 0.6019 - prediction_speakers_accuracy: 0.9074\n",
      "Epoch 9/50\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 1.2343 - prediction_words_loss: 1.0141 - prediction_speakers_loss: 0.2202 - prediction_words_accuracy: 0.7037 - prediction_speakers_accuracy: 0.9259\n",
      "Epoch 10/50\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 1.2320 - prediction_words_loss: 0.9802 - prediction_speakers_loss: 0.2517 - prediction_words_accuracy: 0.6667 - prediction_speakers_accuracy: 0.9537\n",
      "Epoch 11/50\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.7928 - prediction_words_loss: 0.6723 - prediction_speakers_loss: 0.1205 - prediction_words_accuracy: 0.7778 - prediction_speakers_accuracy: 0.9630\n",
      "Epoch 12/50\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 1.2362 - prediction_words_loss: 1.0062 - prediction_speakers_loss: 0.2300 - prediction_words_accuracy: 0.6852 - prediction_speakers_accuracy: 0.9167\n",
      "Epoch 13/50\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.6833 - prediction_words_loss: 0.5343 - prediction_speakers_loss: 0.1490 - prediction_words_accuracy: 0.7870 - prediction_speakers_accuracy: 0.9259\n",
      "Epoch 14/50\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 1.1116 - prediction_words_loss: 0.8775 - prediction_speakers_loss: 0.2341 - prediction_words_accuracy: 0.7130 - prediction_speakers_accuracy: 0.9074\n",
      "Epoch 15/50\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.8715 - prediction_words_loss: 0.6825 - prediction_speakers_loss: 0.1891 - prediction_words_accuracy: 0.7500 - prediction_speakers_accuracy: 0.9537\n",
      "Epoch 16/50\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.8266 - prediction_words_loss: 0.6935 - prediction_speakers_loss: 0.1331 - prediction_words_accuracy: 0.7778 - prediction_speakers_accuracy: 0.9722\n",
      "Epoch 17/50\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.6503 - prediction_words_loss: 0.5001 - prediction_speakers_loss: 0.1503 - prediction_words_accuracy: 0.8241 - prediction_speakers_accuracy: 0.9722\n",
      "Epoch 18/50\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.5417 - prediction_words_loss: 0.4439 - prediction_speakers_loss: 0.0978 - prediction_words_accuracy: 0.8426 - prediction_speakers_accuracy: 0.9444\n",
      "Epoch 19/50\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.6349 - prediction_words_loss: 0.4316 - prediction_speakers_loss: 0.2033 - prediction_words_accuracy: 0.8704 - prediction_speakers_accuracy: 0.9352\n",
      "Epoch 20/50\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.6310 - prediction_words_loss: 0.4730 - prediction_speakers_loss: 0.1580 - prediction_words_accuracy: 0.8333 - prediction_speakers_accuracy: 0.9537\n",
      "Epoch 21/50\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.6176 - prediction_words_loss: 0.4634 - prediction_speakers_loss: 0.1542 - prediction_words_accuracy: 0.8426 - prediction_speakers_accuracy: 0.9352\n",
      "Epoch 22/50\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.5755 - prediction_words_loss: 0.4818 - prediction_speakers_loss: 0.0937 - prediction_words_accuracy: 0.8611 - prediction_speakers_accuracy: 0.9815\n",
      "Epoch 23/50\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.3862 - prediction_words_loss: 0.2777 - prediction_speakers_loss: 0.1085 - prediction_words_accuracy: 0.9074 - prediction_speakers_accuracy: 0.9722\n",
      "Epoch 24/50\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.4656 - prediction_words_loss: 0.3840 - prediction_speakers_loss: 0.0816 - prediction_words_accuracy: 0.8889 - prediction_speakers_accuracy: 0.9630\n",
      "Epoch 25/50\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.5361 - prediction_words_loss: 0.4266 - prediction_speakers_loss: 0.1094 - prediction_words_accuracy: 0.8426 - prediction_speakers_accuracy: 0.9722\n",
      "Epoch 26/50\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.3191 - prediction_words_loss: 0.2650 - prediction_speakers_loss: 0.0541 - prediction_words_accuracy: 0.8889 - prediction_speakers_accuracy: 0.9630\n",
      "Epoch 27/50\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.4003 - prediction_words_loss: 0.3774 - prediction_speakers_loss: 0.0229 - prediction_words_accuracy: 0.9352 - prediction_speakers_accuracy: 0.9907\n",
      "Epoch 28/50\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.4481 - prediction_words_loss: 0.3943 - prediction_speakers_loss: 0.0538 - prediction_words_accuracy: 0.8889 - prediction_speakers_accuracy: 0.9722\n",
      "Epoch 29/50\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.4237 - prediction_words_loss: 0.3387 - prediction_speakers_loss: 0.0850 - prediction_words_accuracy: 0.8981 - prediction_speakers_accuracy: 0.9537\n",
      "Epoch 30/50\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.3934 - prediction_words_loss: 0.3202 - prediction_speakers_loss: 0.0732 - prediction_words_accuracy: 0.9074 - prediction_speakers_accuracy: 0.9815\n",
      "Epoch 31/50\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.3610 - prediction_words_loss: 0.3109 - prediction_speakers_loss: 0.0501 - prediction_words_accuracy: 0.9167 - prediction_speakers_accuracy: 0.9907\n",
      "Epoch 32/50\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.5289 - prediction_words_loss: 0.3885 - prediction_speakers_loss: 0.1404 - prediction_words_accuracy: 0.8704 - prediction_speakers_accuracy: 0.9722\n",
      "Epoch 33/50\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.3797 - prediction_words_loss: 0.2835 - prediction_speakers_loss: 0.0962 - prediction_words_accuracy: 0.9167 - prediction_speakers_accuracy: 0.9537    \n",
      "Epoch 34/50\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.5297 - prediction_words_loss: 0.3318 - prediction_speakers_loss: 0.1980 - prediction_words_accuracy: 0.8796 - prediction_speakers_accuracy: 0.9352\n",
      "Epoch 35/50\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.3385 - prediction_words_loss: 0.2780 - prediction_speakers_loss: 0.0605 - prediction_words_accuracy: 0.9259 - prediction_speakers_accuracy: 0.9722\n",
      "Epoch 36/50\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.2647 - prediction_words_loss: 0.2618 - prediction_speakers_loss: 0.0030 - prediction_words_accuracy: 0.9167 - prediction_speakers_accuracy: 1.0000\n",
      "Epoch 37/50\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.1802 - prediction_words_loss: 0.1354 - prediction_speakers_loss: 0.0448 - prediction_words_accuracy: 0.9352 - prediction_speakers_accuracy: 0.9815\n",
      "Epoch 38/50\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.0738 - prediction_words_loss: 0.0572 - prediction_speakers_loss: 0.0166 - prediction_words_accuracy: 0.9907 - prediction_speakers_accuracy: 0.9907\n",
      "Epoch 39/50\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.1267 - prediction_words_loss: 0.1123 - prediction_speakers_loss: 0.0144 - prediction_words_accuracy: 0.9722 - prediction_speakers_accuracy: 0.9907\n",
      "Epoch 40/50\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.1709 - prediction_words_loss: 0.0659 - prediction_speakers_loss: 0.1051 - prediction_words_accuracy: 0.9722 - prediction_speakers_accuracy: 0.9722\n",
      "Epoch 41/50\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.1789 - prediction_words_loss: 0.1028 - prediction_speakers_loss: 0.0761 - prediction_words_accuracy: 0.9630 - prediction_speakers_accuracy: 0.9815    \n",
      "Epoch 42/50\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.1999 - prediction_words_loss: 0.1376 - prediction_speakers_loss: 0.0623 - prediction_words_accuracy: 0.9537 - prediction_speakers_accuracy: 0.9815\n",
      "Epoch 43/50\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.5401 - prediction_words_loss: 0.3551 - prediction_speakers_loss: 0.1850 - prediction_words_accuracy: 0.9074 - prediction_speakers_accuracy: 0.9444\n",
      "Epoch 44/50\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.4190 - prediction_words_loss: 0.3089 - prediction_speakers_loss: 0.1101 - prediction_words_accuracy: 0.8704 - prediction_speakers_accuracy: 0.9630\n",
      "Epoch 45/50\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.5633 - prediction_words_loss: 0.4474 - prediction_speakers_loss: 0.1159 - prediction_words_accuracy: 0.8426 - prediction_speakers_accuracy: 0.9722\n",
      "Epoch 46/50\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.3653 - prediction_words_loss: 0.3213 - prediction_speakers_loss: 0.0440 - prediction_words_accuracy: 0.8889 - prediction_speakers_accuracy: 0.9907\n",
      "Epoch 47/50\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.3269 - prediction_words_loss: 0.2531 - prediction_speakers_loss: 0.0738 - prediction_words_accuracy: 0.9074 - prediction_speakers_accuracy: 0.9815    \n",
      "Epoch 48/50\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.1375 - prediction_words_loss: 0.0615 - prediction_speakers_loss: 0.0760 - prediction_words_accuracy: 0.9815 - prediction_speakers_accuracy: 0.9630\n",
      "Epoch 49/50\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.2595 - prediction_words_loss: 0.2273 - prediction_speakers_loss: 0.0321 - prediction_words_accuracy: 0.8981 - prediction_speakers_accuracy: 0.9907\n",
      "Epoch 50/50\n",
      "108/108 [==============================] - 0s 1ms/sample - loss: 0.3866 - prediction_words_loss: 0.2432 - prediction_speakers_loss: 0.1434 - prediction_words_accuracy: 0.9167 - prediction_speakers_accuracy: 0.9537\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "history=model.fit(trainX, [trainYwords_cat,trainYspeakers_cat], batch_size=2,nb_epoch=50,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 10, 20)\n",
      "[[1.8516155e-03 2.8807502e-03 1.4103903e-05 8.0832740e-07 6.3417517e-08\n",
      "  2.1912331e-04 3.1062678e-05 8.2082192e-07 9.5103615e-06 3.1980427e-08\n",
      "  9.9484324e-01 7.8451530e-06 4.0716599e-09 4.6922705e-07 4.5425601e-09\n",
      "  1.4017861e-04 3.0485876e-07]] [[1.5889159e-09 2.9108355e-10 2.2578655e-09 1.0000000e+00]]\n",
      "10 = 0.99484324\n",
      "3 = 1.0\n",
      "paco  ha dicho ' para '\n",
      "['avanza' 'buenos-dias' 'casa' 'cinco' 'cuatro' 'dos' 'hola' 'no' 'nueve'\n",
      " 'ocho' 'para' 'retrocede' 'seis' 'si' 'siete' 'tres' 'uno']\n",
      "['carlos' 'eva' 'irene' 'paco']\n"
     ]
    }
   ],
   "source": [
    "print(trainX[5:7,:,:].shape)\n",
    "predWord,predSpeaker=model.predict(trainX[5:6,:,:])\n",
    "print(predWord,predSpeaker)\n",
    "y_predWord   = np.argmax(predWord)\n",
    "y_predSpeaker= np.argmax(predSpeaker)\n",
    "print(y_predWord,\"=\",np.max(predWord))\n",
    "print(y_predSpeaker,\"=\",np.max(predSpeaker))\n",
    "word=ea.leWords.inverse_transform([y_predWord])\n",
    "speaker=ea.leSpeakers.inverse_transform([y_predSpeaker])\n",
    "print(speaker[0],\" ha dicho \\'\",word[0],\"\\'\")\n",
    "print(ea.leWords.classes_)\n",
    "print(ea.leSpeakers.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
