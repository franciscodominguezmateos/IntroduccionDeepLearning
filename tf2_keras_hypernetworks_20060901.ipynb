{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from: https://keras.io/getting_started/intro_to_keras_for_researchers/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HyperNetworks\n",
    "A hypernetwork is a deep neural network whose weights are generated by another network (usually smaller).\n",
    "\n",
    "Let's implement a really trivial hypernetwork: we'll use a small 2-layer network to generate the weights of a larger 3-layer network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 784\n",
    "classes = 10\n",
    "\n",
    "# This is the model we'll actually use to predict labels (the hypernetwork).\n",
    "outer_model = keras.Sequential(\n",
    "    [keras.layers.Dense(64, activation=tf.nn.relu), \n",
    "     keras.layers.Dense(classes),]\n",
    ")\n",
    "\n",
    "# It doesn't need to create its own weights, so let's mark its layers\n",
    "# as already built. That way, calling `outer_model` won't create new variables.\n",
    "for layer in outer_model.layers:\n",
    "    layer.built = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the number of weight coefficients to generate. Each layer in the\n",
    "# hypernetwork requires output_dim * input_dim + output_dim coefficients.\n",
    "num_weights_to_generate = (classes * 64 + classes) + (64 * input_dim + 64)\n",
    "\n",
    "# This is the model that generates the weights of the `outer_model` above.\n",
    "inner_model = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.Dense(16, activation=tf.nn.relu),\n",
    "        keras.layers.Dense(num_weights_to_generate, activation=tf.nn.sigmoid),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer.\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "\n",
    "# Prepare a dataset.\n",
    "(x_train, y_train), _ = tf.keras.datasets.mnist.load_data()\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_train.reshape(60000, 784).astype(\"float32\") / 255, y_train)\n",
    ")\n",
    "\n",
    "# We'll use a batch size of 1 for this experiment.\n",
    "dataset = dataset.shuffle(buffer_size=1024).batch(1)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Predict weights for the outer model.\n",
    "        weights_pred = inner_model(x)\n",
    "\n",
    "        # Reshape them to the expected shapes for w and b for the outer model.\n",
    "        # Layer 0 kernel.\n",
    "        start_index = 0\n",
    "        w0_shape = (input_dim, 64)\n",
    "        w0_coeffs = weights_pred[:, start_index : start_index + np.prod(w0_shape)]\n",
    "        w0 = tf.reshape(w0_coeffs, w0_shape)\n",
    "        start_index += np.prod(w0_shape)\n",
    "        # Layer 0 bias.\n",
    "        b0_shape = (64,)\n",
    "        b0_coeffs = weights_pred[:, start_index : start_index + np.prod(b0_shape)]\n",
    "        b0 = tf.reshape(b0_coeffs, b0_shape)\n",
    "        start_index += np.prod(b0_shape)\n",
    "        # Layer 1 kernel.\n",
    "        w1_shape = (64, classes)\n",
    "        w1_coeffs = weights_pred[:, start_index : start_index + np.prod(w1_shape)]\n",
    "        w1 = tf.reshape(w1_coeffs, w1_shape)\n",
    "        start_index += np.prod(w1_shape)\n",
    "        # Layer 1 bias.\n",
    "        b1_shape = (classes,)\n",
    "        b1_coeffs = weights_pred[:, start_index : start_index + np.prod(b1_shape)]\n",
    "        b1 = tf.reshape(b1_coeffs, b1_shape)\n",
    "        start_index += np.prod(b1_shape)\n",
    "\n",
    "        # Set the weight predictions as the weight variables on the outer model.\n",
    "        outer_model.layers[0].kernel = w0\n",
    "        outer_model.layers[0].bias = b0\n",
    "        outer_model.layers[1].kernel = w1\n",
    "        outer_model.layers[1].bias = b1\n",
    "\n",
    "        # Inference on the outer model.\n",
    "        preds = outer_model(x)\n",
    "        loss = loss_fn(y, preds)\n",
    "\n",
    "    # Train only inner model.\n",
    "    grads = tape.gradient(loss, inner_model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, inner_model.trainable_weights))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 Loss: 4.6243977546691895\n",
      "Step: 100 Loss: 2.6400829122798277\n",
      "Step: 200 Loss: 2.2352249455818933\n",
      "Step: 300 Loss: 2.048253087060816\n",
      "Step: 400 Loss: 1.9136626055450394\n",
      "Step: 500 Loss: 1.779765464322277\n",
      "Step: 600 Loss: 1.7298317057680053\n",
      "Step: 700 Loss: 1.637547222141946\n",
      "Step: 800 Loss: 1.5927312879532527\n",
      "Step: 900 Loss: 1.5260903194025564\n",
      "Step: 1000 Loss: 1.4715679380608213\n",
      "Step: 1100 Loss: 1.4332551930922335\n",
      "Step: 1200 Loss: 1.3952297396319036\n",
      "Step: 1300 Loss: 1.3691044495889846\n",
      "Step: 1400 Loss: 1.343293452130987\n",
      "Step: 1500 Loss: 1.3189049076194086\n",
      "Step: 1600 Loss: 1.3087429432102997\n",
      "Step: 1700 Loss: 1.2856774886782134\n",
      "Step: 1800 Loss: 1.2653956043660983\n",
      "Step: 1900 Loss: 1.2507410198291078\n",
      "Step: 2000 Loss: 1.2228993528042147\n",
      "Step: 2100 Loss: 1.2129999295344809\n",
      "Step: 2200 Loss: 1.196503938620133\n",
      "Step: 2300 Loss: 1.1844519183344342\n",
      "Step: 2400 Loss: 1.1654086992530375\n",
      "Step: 2500 Loss: 1.1604776558167191\n",
      "Step: 2600 Loss: 1.154352454285118\n",
      "Step: 2700 Loss: 1.1478716704938412\n",
      "Step: 2800 Loss: 1.1413472891881062\n",
      "Step: 2900 Loss: 1.1375134859323035\n",
      "Step: 3000 Loss: 1.126769002191364\n",
      "Step: 3100 Loss: 1.1104203180655658\n",
      "Step: 3200 Loss: 1.1001363817733973\n",
      "Step: 3300 Loss: 1.0889693728728593\n",
      "Step: 3400 Loss: 1.0853295405522765\n",
      "Step: 3500 Loss: 1.072026369933401\n",
      "Step: 3600 Loss: 1.0577265082778902\n",
      "Step: 3700 Loss: 1.048585055531199\n",
      "Step: 3800 Loss: 1.040622650081786\n",
      "Step: 3900 Loss: 1.0285060568455389\n",
      "Step: 4000 Loss: 1.0209230528267539\n",
      "Step: 4100 Loss: 1.0113950303718475\n",
      "Step: 4200 Loss: 1.0109542610148603\n",
      "Step: 4300 Loss: 1.0063115059018497\n",
      "Step: 4400 Loss: 0.9984780407896515\n",
      "Step: 4500 Loss: 0.9890198260662373\n",
      "Step: 4600 Loss: 0.9861343385944786\n",
      "Step: 4700 Loss: 0.9818431305008872\n",
      "Step: 4800 Loss: 0.9788554585090752\n",
      "Step: 4900 Loss: 0.9733103982612852\n",
      "Step: 5000 Loss: 0.9609635648888981\n",
      "Step: 5100 Loss: 0.9556854155278702\n",
      "Step: 5200 Loss: 0.9506108444069347\n",
      "Step: 5300 Loss: 0.9464281683900173\n",
      "Step: 5400 Loss: 0.9446446400777452\n",
      "Step: 5500 Loss: 0.9412814082314455\n",
      "Step: 5600 Loss: 0.9373345216576944\n",
      "Step: 5700 Loss: 0.9328312260202468\n",
      "Step: 5800 Loss: 0.9318649758728881\n",
      "Step: 5900 Loss: 0.9282945574637868\n",
      "Step: 6000 Loss: 0.922962614536254\n",
      "Step: 6100 Loss: 0.9176587879407179\n",
      "Step: 6200 Loss: 0.9097162769847716\n",
      "Step: 6300 Loss: 0.9073004553785236\n",
      "Step: 6400 Loss: 0.9009845772501149\n",
      "Step: 6500 Loss: 0.8965047215656107\n",
      "Step: 6600 Loss: 0.8896375970239611\n",
      "Step: 6700 Loss: 0.8854151360505346\n",
      "Step: 6800 Loss: 0.8839656972482024\n",
      "Step: 6900 Loss: 0.8799030490090753\n",
      "Step: 7000 Loss: 0.883900547325338\n",
      "Step: 7100 Loss: 0.882267457922229\n",
      "Step: 7200 Loss: 0.8791793234769508\n",
      "Step: 7300 Loss: 0.8741490672187145\n",
      "Step: 7400 Loss: 0.8693016753802348\n",
      "Step: 7500 Loss: 0.8715403647924429\n",
      "Step: 7600 Loss: 0.8705685202836727\n",
      "Step: 7700 Loss: 0.870919140352923\n",
      "Step: 7800 Loss: 0.8715776695011572\n",
      "Step: 7900 Loss: 0.8721755348507287\n",
      "Step: 8000 Loss: 0.8701344204079933\n",
      "Step: 8100 Loss: 0.8699163116429389\n",
      "Step: 8200 Loss: 0.8686453326464206\n",
      "Step: 8300 Loss: 0.8661118175656444\n",
      "Step: 8400 Loss: 0.8635329224900636\n",
      "Step: 8500 Loss: 0.8600909588424429\n",
      "Step: 8600 Loss: 0.8593435386709332\n",
      "Step: 8700 Loss: 0.8574351484692609\n",
      "Step: 8800 Loss: 0.8549078966320159\n",
      "Step: 8900 Loss: 0.8510508636609994\n",
      "Step: 9000 Loss: 0.8493710780728747\n",
      "Step: 9100 Loss: 0.8471843809935182\n",
      "Step: 9200 Loss: 0.8466617236561638\n",
      "Step: 9300 Loss: 0.8415679897977275\n",
      "Step: 9400 Loss: 0.8379768421824919\n",
      "Step: 9500 Loss: 0.8365775379631223\n",
      "Step: 9600 Loss: 0.8336516829498869\n",
      "Step: 9700 Loss: 0.8330115631299506\n",
      "Step: 9800 Loss: 0.8301082858339233\n",
      "Step: 9900 Loss: 0.8256019605230102\n",
      "Step: 10000 Loss: 0.8207590299812733\n"
     ]
    }
   ],
   "source": [
    "losses = []  # Keep track of the losses over time.\n",
    "for step, (x, y) in enumerate(dataset):\n",
    "    loss = train_step(x, y)\n",
    "\n",
    "    # Logging.\n",
    "    losses.append(float(loss))\n",
    "    if step % 100 == 0:\n",
    "        print(\"Step:\", step, \"Loss:\", sum(losses) / len(losses))\n",
    "\n",
    "    # Stop after 1000 steps.\n",
    "    # Training the model to convergence is left\n",
    "    # as an exercise to the reader.\n",
    "    if step >= 10000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
