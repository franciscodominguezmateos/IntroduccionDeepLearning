{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from: https://www.tensorflow.org/guide/keras/custom_layers_and_models#putting_it_all_together_an_end-to-end_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import InputLayer,Conv2D,Flatten,Dense,Reshape,Conv2DTranspose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(layers.Layer):\n",
    "  \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "  def call(self, inputs):\n",
    "    z_mean, z_log_var = inputs\n",
    "    batch = tf.shape(z_mean)[0]\n",
    "    dim = tf.shape(z_mean)[1]\n",
    "    epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "    return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "\n",
    "class Encoder(layers.Layer):\n",
    "  \"\"\"Maps MNIST digits to a triplet (z_mean, z_log_var, z).\"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               latent_dim=32,\n",
    "               intermediate_dim=64,\n",
    "               name='encoder',\n",
    "               **kwargs):\n",
    "    super(Encoder, self).__init__(name=name, **kwargs)\n",
    "    self.dense_proj = layers.Dense(intermediate_dim, activation='relu')\n",
    "    self.dense_mean = layers.Dense(latent_dim)\n",
    "    self.dense_log_var = layers.Dense(latent_dim)\n",
    "    self.sampling = Sampling()\n",
    "\n",
    "  def call(self, inputs):\n",
    "    x = self.dense_proj(inputs)\n",
    "    z_mean = self.dense_mean(x)\n",
    "    z_log_var = self.dense_log_var(x)\n",
    "    z = self.sampling((z_mean, z_log_var))\n",
    "    return z_mean, z_log_var, z\n",
    "\n",
    "\n",
    "class Decoder(layers.Layer):\n",
    "  \"\"\"Converts z, the encoded digit vector, back into a readable digit.\"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               original_dim,\n",
    "               intermediate_dim=64,\n",
    "               name='decoder',\n",
    "               **kwargs):\n",
    "    super(Decoder, self).__init__(name=name, **kwargs)\n",
    "    self.dense_proj = layers.Dense(intermediate_dim, activation='relu')\n",
    "    self.dense_output = layers.Dense(original_dim, activation='sigmoid')\n",
    "\n",
    "  def call(self, inputs):\n",
    "    x = self.dense_proj(inputs)\n",
    "    return self.dense_output(x)\n",
    "\n",
    "\n",
    "class VariationalAutoEncoder(tf.keras.Model):\n",
    "  \"\"\"Combines the encoder and decoder into an end-to-end model for training.\"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               original_dim,\n",
    "               intermediate_dim=64,\n",
    "               latent_dim=32,\n",
    "               name='autoencoder',\n",
    "               **kwargs):\n",
    "    super(VariationalAutoEncoder, self).__init__(name=name, **kwargs)\n",
    "    self.original_dim = original_dim\n",
    "    self.encoder = Encoder(latent_dim=latent_dim,\n",
    "                           intermediate_dim=intermediate_dim)\n",
    "    self.decoder = Decoder(original_dim, intermediate_dim=intermediate_dim)\n",
    "\n",
    "  def call(self, inputs):\n",
    "    z_mean, z_log_var, z = self.encoder(inputs)\n",
    "    reconstructed = self.decoder(z)\n",
    "    # Add KL divergence regularization loss.\n",
    "    kl_loss = - 0.5 * tf.reduce_mean(\n",
    "        z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1)\n",
    "    self.add_loss(kl_loss)\n",
    "    return reconstructed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dim = 784\n",
    "vae = VariationalAutoEncoder(original_dim, 64, 32)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "mse_loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "#loss_metric = tf.keras.metrics.Mean()\n",
    "\n",
    "(x_train, _), _ = tf.keras.datasets.mnist.load_data()\n",
    "x_train = x_train.reshape(60000, 784).astype('float32') / 255\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(x_train)\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "epochs = 3\n",
    "\n",
    "# Iterate over epochs.\n",
    "for epoch in range(epochs):\n",
    "  print('Start of epoch %d' % (epoch,))\n",
    "\n",
    "  # Iterate over the batches of the dataset.\n",
    "  for step, x_batch_train in enumerate(train_dataset):\n",
    "    with tf.GradientTape() as tape:\n",
    "      reconstructed = vae(x_batch_train)\n",
    "      # Compute reconstruction loss\n",
    "      loss = mse_loss_fn(x_batch_train, reconstructed)\n",
    "      loss += sum(vae.losses)  # Add KLD regularization loss\n",
    "\n",
    "    grads = tape.gradient(loss, vae.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, vae.trainable_weights))\n",
    "\n",
    "    loss_metric(loss)\n",
    "\n",
    "    if step % 100 == 0:\n",
    "      print('step %s: mean loss = %s' % (step, loss_metric.result()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
