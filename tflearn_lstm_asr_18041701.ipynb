{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports doned\n",
      "2\n",
      "wav2mfcc start /home/francisco/voz/recording.wav\n",
      "file read\n",
      "(102350,) 16000\n",
      "(200, 20)\n",
      "[ -1.74761046e+02   3.25462522e+01  -1.28745401e+01   2.07543217e+01\n",
      "   8.96673142e+00   2.17993289e+01   1.51516180e+00   1.65216094e+01\n",
      "  -3.47102561e-01   1.11789198e+01   5.11714193e+00   1.60621764e+01\n",
      "   3.92731608e+00   1.34172782e+01   2.18723316e+00   1.05491680e+01\n",
      "   1.30924433e+01   6.29194202e+00  -9.30889407e-02   4.55653326e+00]\n",
      "filling holes\n",
      "(300, 20)\n",
      "[-213.71106211   61.51051599  -47.66622437    1.79295504   14.38711338\n",
      "   38.0233543     9.56786974   22.73441325   14.6864944    17.19112725\n",
      "   -2.69500166   15.10978483   16.09496528   19.61873191   -6.28172785\n",
      "   -0.48229283    7.3208475    17.82623085   11.93250591    5.19791082]\n",
      "wav2mfcc start /home/francisco/voz/nada.wav\n",
      "file read\n",
      "(21632,) 16000\n",
      "(43, 20)\n",
      "[-154.04626133  121.17766382  -24.79870298  -11.80892048  -30.26455887\n",
      "  -18.6659575     2.7918911     8.70540668   -2.57157187   -9.93394808\n",
      "   -7.79782125   -7.18287772    1.54235194   -3.62298276   -3.74948981\n",
      "  -10.31970044   -7.62944964   -4.79090675   -1.34671025   -3.33313681]\n",
      "filling holes\n",
      "(300, 20)\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Simple example using LSTM recurrent neural network to classify IMDB\n",
    "sentiment dataset.\n",
    "References:\n",
    "    - Long Short Term Memory, Sepp Hochreiter & Jurgen Schmidhuber, Neural\n",
    "    Computation 9(8): 1735-1780, 1997.\n",
    "    - Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng,\n",
    "    and Christopher Potts. (2011). Learning Word Vectors for Sentiment\n",
    "    Analysis. The 49th Annual Meeting of the Association for Computational\n",
    "    Linguistics (ACL 2011).\n",
    "Links:\n",
    "    - http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf\n",
    "    - http://ai.stanford.edu/~amaas/data/sentiment/\n",
    "\"\"\"\n",
    "from __future__ import division, print_function, absolute_import\n",
    "import os\n",
    "import numpy as np\n",
    "import sklearn.preprocessing as prep\n",
    "import librosa\n",
    "import tflearn\n",
    "from tflearn.data_utils import to_categorical, pad_sequences\n",
    "from tflearn.datasets import imdb\n",
    "\n",
    "# IMDB Dataset loading\n",
    "#train, test, _ = imdb.load_data(path='imdb.pkl', n_words=10000,\n",
    "#                                valid_portion=0.1)\n",
    "\n",
    "print(\"Imports doned\")\n",
    "def wav2mfcc(file_path, max_pad_len=300):\n",
    "    print(\"wav2mfcc start\",file_path)\n",
    "    wave, sr = librosa.load(file_path, mono=True, sr=None)\n",
    "    print(\"file read\")\n",
    "    print(wave.shape,sr)\n",
    "    wave=prep.maxabs_scale(wave) #Normalize amplitude to +-1\n",
    "    mfcc = librosa.feature.mfcc(wave, sr=sr).transpose()\n",
    "    print(mfcc.shape)\n",
    "    print(mfcc[1,:])\n",
    "    dif=max_pad_len-mfcc.shape[0]\n",
    "    #padding sample\n",
    "    if dif>0:\n",
    "        #fill holes\n",
    "        print(\"filling holes\")\n",
    "        for i in range(dif):\n",
    "            mfcc=np.vstack([mfcc,np.zeros(mfcc.shape[1])])\n",
    "    else:\n",
    "        #take away excess\n",
    "        print(\"excess\")\n",
    "        mfcc=mfcc[:max_pad_len]\n",
    "    print(mfcc.shape)\n",
    "    print(mfcc[199,:])\n",
    "    return mfcc\n",
    "\n",
    "class EasyASR(object):\n",
    "    '''\n",
    "    classdocs\n",
    "    '''\n",
    "\n",
    "    def __init__(self, filePath=\"/home/francisco/voz\"):\n",
    "        '''\n",
    "        Constructor\n",
    "        '''\n",
    "        basepath=filePath\n",
    "        self.mfccs={}\n",
    "        mfccAll=[]\n",
    "        labels=os.listdir(basepath)\n",
    "        print(len(labels))\n",
    "        for i,nn in enumerate(labels):\n",
    "            n=nn.split(\".\")[0]#take away extension .wav\n",
    "            self.mfccs[n] = wav2mfcc(basepath+'/{}.wav'.format(n))\n",
    "ea=EasyASR()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17, 25, 10, 406, 26, 14, 56, 61, 62, 323, 4] 0\n",
      "(22500, 100) (22500, 2)\n"
     ]
    }
   ],
   "source": [
    "trainX, trainY = train\n",
    "testX, testY = test\n",
    "print (trainX[0],trainY[0])\n",
    "\n",
    "# Data preprocessing\n",
    "# Sequence padding\n",
    "trainX = pad_sequences(trainX, maxlen=100, value=0.)\n",
    "testX = pad_sequences(testX, maxlen=100, value=0.)\n",
    "# Converting labels to binary vectors\n",
    "trainY = to_categorical(trainY,2)\n",
    "testY = to_categorical(testY,2)\n",
    "print (trainX.shape,trainY.shape)\n",
    "\n",
    "# Network building\n",
    "net = tflearn.input_data([None, 100])\n",
    "net = tflearn.embedding(net, input_dim=10000, output_dim=128)\n",
    "net = tflearn.lstm(net, 128, dropout=0.8, return_seq=True)\n",
    "net = tflearn.lstm(net, 128, dropout=0.8)\n",
    "net = tflearn.fully_connected(net, 2, activation='softmax')\n",
    "net = tflearn.regression(net, optimizer='adam', learning_rate=0.001,\n",
    "                         loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 7039  | total loss: \u001b[1m\u001b[32m0.12234\u001b[0m\u001b[0m | time: 58.578s\n",
      "| Adam | epoch: 010 | loss: 0.12234 - acc: 0.9629 -- iter: 22496/22500\n",
      "Training Step: 7040  | total loss: \u001b[1m\u001b[32m0.12032\u001b[0m\u001b[0m | time: 60.111s\n",
      "| Adam | epoch: 010 | loss: 0.12032 - acc: 0.9635 | val_loss: 0.57867 - val_acc: 0.7936 -- iter: 22500/22500\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "model = tflearn.DNN(net, tensorboard_verbose=0)\n",
    "model.fit(trainX, trainY, validation_set=(testX, testY), show_metric=True,\n",
    "batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
